<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>MT Categories | Learning and Language Blog</title>
    <meta name="generator" content="VuePress 1.9.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.acdc4f47.css" as="style"><link rel="preload" href="/blog/assets/js/app.1e476687.js" as="script"><link rel="preload" href="/blog/assets/js/8.d0eaa22e.js" as="script"><link rel="preload" href="/blog/assets/js/1.4a2adf96.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.14505170.js"><link rel="prefetch" href="/blog/assets/js/11.d4adc340.js"><link rel="prefetch" href="/blog/assets/js/12.c3707bf6.js"><link rel="prefetch" href="/blog/assets/js/13.b12c1fa7.js"><link rel="prefetch" href="/blog/assets/js/14.6b8658a9.js"><link rel="prefetch" href="/blog/assets/js/15.6ffbe258.js"><link rel="prefetch" href="/blog/assets/js/16.f43ed359.js"><link rel="prefetch" href="/blog/assets/js/17.4308b200.js"><link rel="prefetch" href="/blog/assets/js/18.751d108a.js"><link rel="prefetch" href="/blog/assets/js/19.48518c5e.js"><link rel="prefetch" href="/blog/assets/js/20.1eae6522.js"><link rel="prefetch" href="/blog/assets/js/21.9bc61d6d.js"><link rel="prefetch" href="/blog/assets/js/22.9286c618.js"><link rel="prefetch" href="/blog/assets/js/23.f43e6fc8.js"><link rel="prefetch" href="/blog/assets/js/24.0c9e0ca4.js"><link rel="prefetch" href="/blog/assets/js/25.c0cff29d.js"><link rel="prefetch" href="/blog/assets/js/26.a182b8e5.js"><link rel="prefetch" href="/blog/assets/js/27.1a5dbb38.js"><link rel="prefetch" href="/blog/assets/js/28.f9c0811c.js"><link rel="prefetch" href="/blog/assets/js/29.55181991.js"><link rel="prefetch" href="/blog/assets/js/3.2ef92242.js"><link rel="prefetch" href="/blog/assets/js/30.148ad523.js"><link rel="prefetch" href="/blog/assets/js/31.2485d125.js"><link rel="prefetch" href="/blog/assets/js/4.ae2a30de.js"><link rel="prefetch" href="/blog/assets/js/5.897bac6b.js"><link rel="prefetch" href="/blog/assets/js/6.8a461025.js"><link rel="prefetch" href="/blog/assets/js/7.4372ae5f.js"><link rel="prefetch" href="/blog/assets/js/9.d4d8a32c.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.acdc4f47.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container categories-wrapper no-sidebar" data-v-130b300a data-v-1e68bc8d><div data-v-130b300a><div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-130b300a data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Learning and Language Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-130b300a><header class="navbar" data-v-130b300a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Learning and Language Blog" class="logo"> <span class="site-name">Learning and Language Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" aria-current="page" class="nav-link router-link-exact-active router-link-active"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-130b300a></div> <aside class="sidebar" data-v-130b300a><div class="personal-info-wrapper" data-v-39576ba9 data-v-130b300a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    Lei Li
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>21</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>39</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" aria-current="page" class="nav-link router-link-exact-active router-link-active"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>MT Categories</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-130b300a><ul class="category-wrapper" style="display:none;" data-v-1e68bc8d data-v-1e68bc8d><li class="category-item" data-v-1e68bc8d><a href="/blog/categories/ST/" data-v-1e68bc8d><span class="category-name" data-v-1e68bc8d>ST</span> <span class="post-num" style="background-color:#f8b26a;" data-v-1e68bc8d>2</span></a></li><li class="category-item" data-v-1e68bc8d><a href="/blog/categories/DL4MT/" data-v-1e68bc8d><span class="category-name" data-v-1e68bc8d>DL4MT</span> <span class="post-num" style="background-color:#abbd81;" data-v-1e68bc8d>15</span></a></li><li class="category-item active" data-v-1e68bc8d><a href="/blog/categories/MT/" aria-current="page" class="router-link-exact-active router-link-active" data-v-1e68bc8d><span class="category-name" data-v-1e68bc8d>MT</span> <span class="post-num" style="background-color:#3498db;" data-v-1e68bc8d>14</span></a></li><li class="category-item" data-v-1e68bc8d><a href="/blog/categories/NLG/" data-v-1e68bc8d><span class="category-name" data-v-1e68bc8d>NLG</span> <span class="post-num" style="background-color:#f8b26a;" data-v-1e68bc8d>1</span></a></li><li class="category-item" data-v-1e68bc8d><a href="/blog/categories/NLP/" data-v-1e68bc8d><span class="category-name" data-v-1e68bc8d>NLP</span> <span class="post-num" style="background-color:#67cc86;" data-v-1e68bc8d>3</span></a></li><li class="category-item" data-v-1e68bc8d><a href="/blog/categories/IE/" data-v-1e68bc8d><span class="category-name" data-v-1e68bc8d>IE</span> <span class="post-num" style="background-color:#e15b64;" data-v-1e68bc8d>1</span></a></li></ul> <div class="abstract-wrapper list" style="display:none;" data-v-6cc0658a data-v-1e68bc8d data-v-1e68bc8d><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/mt/VOLT/" data-v-ff2c8be0>Learning Optimal Vocabularies for Machine Translation in 30 Minutes</a></div> <div class="abstract" data-v-ff2c8be0><p>Constructing a vocabulary is a fisrt step for any NLP tasks.
How can we efficiently learn an optimal vocabulary for machine translation?
In this blog, I will explain the VOLT algorithm from the paper <em>Vocabulary Leaning via Optimal Transport for Neural Machine Translation</em>, which was awarded the best paper at ACL 2021.</p>
<p>Reading time: About 8 minutes</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Ahmed Elkordy</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>5/17/2022</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Multilingual MT</span><span class="tag-item" data-v-f875f3fc>Vocabulary Learning</span><span class="tag-item" data-v-f875f3fc>Optimal Transport</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/" data-v-ff2c8be0>Neural Machine Translation with Monolingual Translation Memory</a></div> <div class="abstract" data-v-ff2c8be0></div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Rajan Saini</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>12/8/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Translation Memory</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/self-training/" data-v-ff2c8be0>Revisiting Self-training for Neural Sequence Generation</a></div> <div class="abstract" data-v-ff2c8be0><p>Self-training is a very prevalent semi-supervised method. Its key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the <em>pseudo-parallel</em> data). Self-training has been widely used in classification tasks. However, will it work on sequence generation tasks (e.g. machine translation)? If so, how does it work? This blog introduces a work [1] which investigates these questions and gives the answers.</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Zekun Li</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>12/5/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Self-training</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/comet/" data-v-ff2c8be0>Automatic Machine Translation Evaluation - COMET Explained</a></div> <div class="abstract" data-v-ff2c8be0></div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Xinyi Wang</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>12/1/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>MT Evaluation</span><span class="tag-item" data-v-f875f3fc>Pre-trained Language Model</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/recurrent-attention/" data-v-ff2c8be0>Recurrent Attention for Neural Machine Translation</a></div> <div class="abstract" data-v-ff2c8be0><p>​Upon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Jiachen Li</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/29/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Transformer</span><span class="tag-item" data-v-f875f3fc>Recurrent Attention</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/cpgmt/" data-v-ff2c8be0>Contextual Parameter Generation for Universal Neural Machine Translation</a></div> <div class="abstract" data-v-ff2c8be0></div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Bairu Hou</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/28/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Multilingual MT</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/unsupervisedmt/" data-v-ff2c8be0>Unsupervised Machine Translation using Monolingual Corpora Only</a></div> <div class="abstract" data-v-ff2c8be0><p>Can one build a neural machine translation model without parallel data?</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Qiucheng Wu</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/28/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Unsupervised Machine Translation</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/mgnmt/" data-v-ff2c8be0>Mirror-Generative Neural Machine Translation</a></div> <div class="abstract" data-v-ff2c8be0><p>In general, neural machine translation (NMT) requires a large amount of parallel data (e.g., EN-&gt;CN). However, it is not easy to collect enough high-quality parallelly-paired sentences for training the translation model. On the other hand, we can capture enormous plain text from Wikipedia or news articles for each specific language. In this paper, MGNMT tries to make good use of non-parallel data and boost the performance of NMT.</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Tsu-Jui Fu</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/25/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Variational Inference</span><span class="tag-item" data-v-f875f3fc>Latent Variable Model</span><span class="tag-item" data-v-f875f3fc>Semi-supervised Machine Translation</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/imigit/" data-v-ff2c8be0>Can Visual Imagination Help Machine Translation?</a></div> <div class="abstract" data-v-ff2c8be0><p>Machine translation has helped people daily life, and is also an important research topic especially in computer science community.
It expands from one language translate to another language, speech translate to text, etc.
Today, I'm going to talk about a paper &quot;Generative Imagination Elevates Machine Translation&quot;. I'll cover the background, challenge and motivation behind this paper. Then I'll go through some technical details of this paper as well as some in-depth analysis of their experimental settings and results. Finally, we will discuss about the potential extension of this work. Hopefully, this would give you a better understanding of this area, and point out to a promising research direction.</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Yujie Lu</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/21/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Imagination</span><span class="tag-item" data-v-f875f3fc>Visual Machine Translation</span><span class="tag-item" data-v-f875f3fc>ImagiT</span></i></div></div><div class="abstract-item" data-v-ff2c8be0 data-v-6cc0658a><!----> <div class="title" data-v-ff2c8be0><!----> <a href="/blog/blogs/dl4mt/2021/mrasp2/" data-v-ff2c8be0>Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</a></div> <div class="abstract" data-v-ff2c8be0><p>How to develop a single unified model to translate from any language to any language?
This work proposes a many-to-many translation system with emphasis on both English-centric and non-English directions. Many recent works have focused on proposing a single unified model for multiligual translation. These models are favorable because they are efficient and easy for deployment. However, most of these works focus on improving English-centric directions, which means that translation between two arbitrary languages may not be well supported. Therefore, in this paper, they propose a training method called mRASP2, including contrastive learning and alignment augmentation (AA) to train a unified multilingual translation system.  They also contribute a monolingual dataset called MC24. By making use of monolingual and bilingual language copora, the system is able to learn language-agnostic representation to support non-English directions better than before. Their system achieves great performances and outperforms a strong Transformer baseline by a large margin.</p>
</div> <div data-v-f875f3fc data-v-ff2c8be0><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Weixi Feng</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/20/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Multilingual MT</span><span class="tag-item" data-v-f875f3fc>Contrastive Learning</span><span class="tag-item" data-v-f875f3fc>Zero-shot Translation</span><span class="tag-item" data-v-f875f3fc>mRASP</span></i></div></div> <div class="pagation pagation" data-v-094d08e4 data-v-6cc0658a><div class="pagation-list" data-v-094d08e4><span unselectable="on" class="jump" style="display:none;" data-v-094d08e4>Prev</span> <span class="jump" style="display:none;" data-v-094d08e4>1</span> <span class="ellipsis" style="display:none;" data-v-094d08e4>...</span> <span class="jump bgprimary" data-v-094d08e4>1</span><span class="jump" data-v-094d08e4>2</span> <span class="ellipsis" style="display:none;" data-v-094d08e4>...</span> <span class="jump" style="display:none;" data-v-094d08e4>2</span> <span class="jump" style="display:;" data-v-094d08e4>Next</span> <span class="jumppoint" data-v-094d08e4>Jump To</span> <span class="jumpinp" data-v-094d08e4><input type="text" value="" data-v-094d08e4></span> <span class="jump gobtn" data-v-094d08e4>Go</span></div></div></div></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.1e476687.js" defer></script><script src="/blog/assets/js/8.d0eaa22e.js" defer></script><script src="/blog/assets/js/1.4a2adf96.js" defer></script>
  </body>
</html>
