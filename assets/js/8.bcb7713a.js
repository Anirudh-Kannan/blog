(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{591:function(e,t,a){e.exports=a.p+"assets/img/lightseqlogo.819d5933.png"},592:function(e,t,a){e.exports=a.p+"assets/img/feature_table.42ca132e.png"},593:function(e,t,a){e.exports=a.p+"assets/img/Transformer.40076700.png"},594:function(e,t,a){e.exports=a.p+"assets/img/fusion.de07cf52.png"},595:function(e,t,a){e.exports=a.p+"assets/img/softmax.a3130038.png"},596:function(e,t,a){e.exports=a.p+"assets/img/training_speed.20825958.png"},597:function(e,t,a){e.exports=a.p+"assets/img/inference_speed.32cb359c.png"},598:function(e,t,a){e.exports=a.p+"assets/img/beamsearch.87c3c841.png"},599:function(e,t,a){e.exports=a.p+"assets/img/beamsearchp4.a8927aa8.png"},600:function(e,t,a){e.exports=a.p+"assets/img/samplingp4.541e46a0.png"},601:function(e,t,a){e.exports=a.p+"assets/img/realworkload.2e0249b8.png"},713:function(e,t,a){"use strict";a.r(t);var r=a(13),n=Object(r.a)({},(function(){var e=this,t=e.$createElement,r=e._self._c||t;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("p",[e._v("A high performance open-source library for NLP Transformer model training and inferencing.\n")]),e._v(" "),r("h2",{attrs:{id:"_1-what-is-lightseq"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-what-is-lightseq"}},[e._v("#")]),e._v(" 1. What is LightSeq?")]),e._v(" "),r("p",[r("img",{attrs:{src:a(591),alt:"logo"}})]),e._v(" "),r("h3",{attrs:{id:"_1-1-nlp-models"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-nlp-models"}},[e._v("#")]),e._v(" 1.1 NLP models")]),e._v(" "),r("p",[e._v("Transformers[1], BERT[2], or GPT[3] models are state-of-art models on natural language processing tasks. They are heavily used and breaking multiple records on sequence-to-sequence (Seq2Seq) tasks including machine translation, text summarization, and text generation, or even computer vision tasks by Vision Transformers (an image is just a sequence of pixels). However, those models are huge in size that needs large-scale training and inference. This makes it computationally expensive, so serving these models is a challenge for real industrial applications.")]),e._v(" "),r("h3",{attrs:{id:"_1-2-motivation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-motivation"}},[e._v("#")]),e._v(" 1.2 Motivation")]),e._v(" "),r("p",[e._v("Due to the high complexity and large parameter size of transformer models, the latency for both training and inference is high. Here are the three comparisons between the current inference systems and LightSeq, and the reasons why they are not able to perform well for online tasks.")]),e._v(" "),r("ol",[r("li",[e._v("Popular deep learning frameworks. Since those models have flexible model structures, both TensorFlow and PyTorch need additional memory allocation and extra overhead for training. Thus, they do not make full use of the hardware resource.")]),e._v(" "),r("li",[e._v("Inference optimization frameworks. Optimization frameworks like TensorFlow XLA, TVM, and TensorRT are not suitable for variable-length inputs, which require dynamic memory allocation that does not perform well for transformer models.")]),e._v(" "),r("li",[e._v("Similar acceleration frameworks. Faster-Transformer and TurboTransformers are similar to LightSeq. However, they do not have all components or features compared to LightSeq (Table 1).")])]),e._v(" "),r("h3",{attrs:{id:"_1-3-lightseq"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-lightseq"}},[e._v("#")]),e._v(" 1.3 Lightseq")]),e._v(" "),r("p",[e._v("LightSeq[4], is a high-performance open-source library for both training and inference that is directly built on top of CUDA official libraries ("),r("a",{attrs:{href:"https://docs.nvidia.com/cuda/cublas/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("cuBLAS"),r("OutboundLink")],1),e._v(", "),r("a",{attrs:{href:"https://docs.nvidia.com/cuda/thrust/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Thrust"),r("OutboundLink")],1),e._v(", "),r("a",{attrs:{href:"http://nvlabs.github.io/cub/",target:"_blank",rel:"noopener noreferrer"}},[e._v("CUB"),r("OutboundLink")],1),e._v(").\nIt supports models in the Transformer family including BERT, GPT, and full encoder-decoder.\nIt introduces new transformer encoders and decoders components after fusing and optimizing the existing models.")]),e._v(" "),r("p",[e._v("The applications of LightSeq include Machine Translation, Text Generation, Dialog, Language Modelling, Sentiment Analysis, and other related tasks with sequence data, which can be easily deployed to commercial products.")]),e._v(" "),r("p",[e._v("LightSeq improves the speed for both training and inference stages. Models like DeepSpeed[5] only accelerate the training, and tools like TensorRT,  FasterTransformer, or TurboTransformers only support optimizing the inference. Here are the comparison tables on different features between LightSeq and other models.")]),e._v(" "),r("p",[r("img",{attrs:{src:a(592),alt:"Features"}})]),e._v(" "),r("p",[e._v("Table 1. The tables above are from the "),r("a",{attrs:{href:"https://github.com/bytedance/lightseq",target:"_blank",rel:"noopener noreferrer"}},[e._v("official Github repository"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("h2",{attrs:{id:"_2-technique-details"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-technique-details"}},[e._v("#")]),e._v(" 2. Technique Details")]),e._v(" "),r("p",[e._v("There are three main methods that LightSeq uses to optimize the model, training speed, and inference speed. The image below shows the architecture of a sequence-to-sequence model using transformers.")]),e._v(" "),r("p",[r("img",{attrs:{src:a(593),alt:"Transformer"}})]),e._v(" "),r("h3",{attrs:{id:"_2-1-operation-fusion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-operation-fusion"}},[e._v("#")]),e._v(" 2.1 Operation Fusion")]),e._v(" "),r("p",[e._v("Transformers model implemented by popular deep learning frameworks like Pytorch or Tensorflow just combine multiple fine-grained kernel functions for one layer. In this way, it needs to launch more kernel functions and uses lots of memory I/O that costs extra time for training and inference.")]),e._v(" "),r("p",[e._v("LightSeq uses general matrix multiply (GEMM) and custom kernel functions, so here are only six custom kernel functions and six GEMM in a Transformer encoder layer for LightSeq models. The right image shows the model structure of the LightSeq transformer encoder layer.")]),e._v(" "),r("p",[r("em",[e._v("Need to add more intuitive description here")]),e._v(" "),r("img",{attrs:{src:a(594),alt:"Fusion"}})]),e._v(" "),r("h3",{attrs:{id:"_2-2-hierarchical-auto-regressive-search"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-hierarchical-auto-regressive-search"}},[e._v("#")]),e._v(" 2.2 Hierarchical Auto-Regressive Search")]),e._v(" "),r("p",[e._v("Searching usually happens in the last step of a transformer model. Redundant calculations often exist in output layers since we only need a few labels/tokens with the highest probability instead of all of them.")]),e._v(" "),r("p",[e._v("LightSeq optimizes this process by using a Hierarchical Auto Regressive Search method to erase redundant calculations and perform parallel computing illustrated as below (using beam search as an example).")]),e._v(" "),r("p",[e._v("the following steps happen for each beam.")]),e._v(" "),r("ol",[r("li",[e._v("Randomly divide logits into k groups")]),e._v(" "),r("li",[e._v("Calculate the maximum of group "),r("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[r("svg",{staticStyle:{"vertical-align":"-0.025ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"0.781ex",height:"1.52ex",viewBox:"0 -661 345 672"}},[r("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[r("g",{attrs:{"data-mml-node":"math"}},[r("g",{attrs:{"data-mml-node":"mi"}},[r("path",{attrs:{"data-c":"69",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])])])])]),e._v(", denoted\nas "),r("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[r("svg",{staticStyle:{"vertical-align":"-0.357ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.651ex",height:"1.357ex",viewBox:"0 -442 1172 599.8"}},[r("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[r("g",{attrs:{"data-mml-node":"math"}},[r("g",{attrs:{"data-mml-node":"msub"}},[r("g",{attrs:{"data-mml-node":"mi"}},[r("path",{attrs:{"data-c":"6D",d:"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"}})]),r("g",{attrs:{"data-mml-node":"mi",transform:"translate(878, -150) scale(0.707)"}},[r("path",{attrs:{"data-c":"69",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])])])])])])],1),e._v(" "),r("li",[e._v("Calculate the minimum of "),r("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[r("svg",{staticStyle:{"vertical-align":"-0.357ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.651ex",height:"1.357ex",viewBox:"0 -442 1172 599.8"}},[r("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[r("g",{attrs:{"data-mml-node":"math"}},[r("g",{attrs:{"data-mml-node":"msub"}},[r("g",{attrs:{"data-mml-node":"mi"}},[r("path",{attrs:{"data-c":"6D",d:"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"}})]),r("g",{attrs:{"data-mml-node":"mi",transform:"translate(878, -150) scale(0.707)"}},[r("path",{attrs:{"data-c":"69",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"}})])])])])])]),e._v(", denoted as "),r("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[r("svg",{staticStyle:{"vertical-align":"-0.048ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.717ex",height:"1.593ex",viewBox:"0 -683 759 704"}},[r("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[r("g",{attrs:{"data-mml-node":"math"}},[r("g",{attrs:{"data-mml-node":"mi"}},[r("path",{attrs:{"data-c":"52",d:"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"}})])])])])]),e._v(",\nwhich can be regarded as a rough top-k value of logits.")],1),e._v(" "),r("li",[e._v("Select logits larger than "),r("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[r("svg",{staticStyle:{"vertical-align":"-0.048ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.717ex",height:"1.593ex",viewBox:"0 -683 759 704"}},[r("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[r("g",{attrs:{"data-mml-node":"math"}},[r("g",{attrs:{"data-mml-node":"mi"}},[r("path",{attrs:{"data-c":"52",d:"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"}})])])])])]),e._v(" and write them into GPU memory.")],1)]),e._v(" "),r("p",[r("img",{attrs:{src:a(595),alt:"softmax"}})]),e._v(" "),r("h3",{attrs:{id:"_2-3-dynamic-gpu-memory-reuse"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-dynamic-gpu-memory-reuse"}},[e._v("#")]),e._v(" 2.3 Dynamic GPU Memory Reuse")]),e._v(" "),r("p",[e._v("LightSeq pre-defines the maximum of dynamic shapes, such as the maximal sequence length, to avoid memory allocation time and save GPU memory occupancy. Also, GPU memory is shared for non-dependent intermediate results to reduce the memory usage.")]),e._v(" "),r("p",[e._v("By using LightSeq, users are able to 8 Transformer big models simultaneously on a NVIDIA Tesla T4 GPU.")]),e._v(" "),r("h2",{attrs:{id:"_3-using-lightseq"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-using-lightseq"}},[e._v("#")]),e._v(" 3. Using LightSeq")]),e._v(" "),r("p",[e._v("Running LightSeq requires one or more GPUs.")]),e._v(" "),r("h3",{attrs:{id:"_3-1-installation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-installation"}},[e._v("#")]),e._v(" 3.1 Installation")]),e._v(" "),r("p",[e._v("LightSeq installation from PyPI only supports python 3.6 to 3.8 on Linux for now. Consider compiling from source if you have other environments.")]),e._v(" "),r("p",[r("code",[e._v("pip install lightseq fairseq sacremoses transformers")])]),e._v(" "),r("h3",{attrs:{id:"_3-2-training-examples-using-lightseq"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-training-examples-using-lightseq"}},[e._v("#")]),e._v(" 3.2 Training examples using LightSeq")]),e._v(" "),r("p",[e._v("Training a translation task on wmt14 en2de dataset by running the following command.")]),e._v(" "),r("p",[r("code",[e._v("sh examples/training/fairseq/ls_fairseq_wmt14en2de.sh")])]),e._v(" "),r("p",[e._v("If you want to run the training using FairSeq, run the following command.")]),e._v(" "),r("p",[r("code",[e._v("sh examples/training/fairseq/fairseq_wmt14en2de.sh")])]),e._v(" "),r("h3",{attrs:{id:"_3-3-inference-examples-using-lightseq"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-inference-examples-using-lightseq"}},[e._v("#")]),e._v(" 3.3 Inference examples Using LightSeq")]),e._v(" "),r("p",[r("code",[e._v("pip install torch tensorflow transformers lightseq")])]),e._v(" "),r("p",[r("code",[e._v("cd examples/inference/python")])]),e._v(" "),r("p",[r("code",[e._v("python export/hf_bart_export.py")])]),e._v(" "),r("p",[r("code",[e._v("python test/ls_bart.py")])]),e._v(" "),r("p",[r("a",{attrs:{href:"https://github.com/bytedance/lightseq/blob/master/docs/guide.md",target:"_blank",rel:"noopener noreferrer"}},[e._v("Here"),r("OutboundLink")],1),e._v(" is a guide on using LightSeq for training and inference.")]),e._v(" "),r("h2",{attrs:{id:"_4-performance"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-performance"}},[e._v("#")]),e._v(" 4. Performance")]),e._v(" "),r("h3",{attrs:{id:"_4-1-training-performance"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-training-performance"}},[e._v("#")]),e._v(" 4.1 Training Performance")]),e._v(" "),r("p",[e._v("The plots below are the experiment results on WMT14 English to German translation tasks using Transformer-Big models. In all plots, FairSeq+LightSeq models are able to improve the performance to 3.5X maximum.")]),e._v(" "),r("p",[r("img",{attrs:{src:a(596),alt:"Training"}})]),e._v(" "),r("p",[e._v("The image above is from the "),r("a",{attrs:{href:"https://github.com/bytedance/lightseq",target:"_blank",rel:"noopener noreferrer"}},[e._v("official Github repository"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("h3",{attrs:{id:"_4-2-inference-performance"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-inference-performance"}},[e._v("#")]),e._v(" 4.2 Inference Performance")]),e._v(" "),r("p",[e._v("Here are the inference results using LightSeq, TensorFlow, PyTorch, and FasterTransformer on neural machine translation using Transformer-base models with beam search methods.")]),e._v(" "),r("p",[r("img",{attrs:{src:a(597),alt:"Inference"}})]),e._v(" "),r("p",[e._v("The image above is from the "),r("a",{attrs:{href:"https://github.com/bytedance/lightseq",target:"_blank",rel:"noopener noreferrer"}},[e._v("official Github repository"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("h3",{attrs:{id:"_4-3-more-inference-performance-on-nvidia-p4-and-t4"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-more-inference-performance-on-nvidia-p4-and-t4"}},[e._v("#")]),e._v(" 4.3 More Inference Performance on Nvidia P4 and T4")]),e._v(" "),r("p",[e._v("The three images below are from the "),r("a",{attrs:{href:"https://segmentfault.com/a/1190000038523998",target:"_blank",rel:"noopener noreferrer"}},[e._v("Volctrans Blog on segmentfault.com"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("p",[e._v("X-axes are the Batch size and sequence length pairs, and Y-axes are the acceleration rates.")]),e._v(" "),r("p",[r("img",{attrs:{src:a(598),alt:"Beam Search T4"}})]),e._v(" "),r("p",[r("img",{attrs:{src:a(599),alt:"Beam Search P4"}})]),e._v(" "),r("p",[r("img",{attrs:{src:a(600),alt:"Sampling"}})]),e._v(" "),r("h3",{attrs:{id:"_4-4-real-world-cloud-computing-delay-test-on-gpt"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-real-world-cloud-computing-delay-test-on-gpt"}},[e._v("#")]),e._v(" 4.4 Real-world Cloud Computing Delay Test on GPT")]),e._v(" "),r("p",[e._v("This plot shows the performance of deploying a GPT model to cloud computing. At 11:00, the delay performance decreased from 360 ms to 80 ms when LightSeq is turned on.")]),e._v(" "),r("p",[r("img",{attrs:{src:a(601),alt:"Real Work Load"}})]),e._v(" "),r("p",[e._v("The image above is from the "),r("a",{attrs:{href:"https://segmentfault.com/a/1190000038523998",target:"_blank",rel:"noopener noreferrer"}},[e._v("Volctrans Blog on segmentfault.com"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("h2",{attrs:{id:"_5-reference"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_5-reference"}},[e._v("#")]),e._v(" 5. Reference")]),e._v(" "),r("p",[e._v('[1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.')]),e._v(" "),r("p",[e._v('[2] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." '),r("em",[e._v("arXiv preprint arXiv:1810.04805")]),e._v(" (2018).")]),e._v(" "),r("p",[e._v('[3] Brown, Tom B., et al. "Language models are few-shot learners." arXiv preprint arXiv:2005.14165 (2020).')]),e._v(" "),r("p",[e._v('[4] Wang, Xiaohui, et al. "LightSeq: A High Performance Inference Library for Transformers." arXiv preprint arXiv:2010.13887 (2020).')]),e._v(" "),r("p",[e._v("[5] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. "),r("a",{attrs:{href:"https://dl.acm.org/doi/10.1145/3394486.3406703",target:"_blank",rel:"noopener noreferrer"}},[e._v("In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)"),r("OutboundLink")],1),e._v(".")])])}),[],!1,null,null,null);t.default=n.exports}}]);