export const data = JSON.parse("{\"key\":\"v-60cbfb81\",\"path\":\"/dl4mt/2021/lut/\",\"title\":\"Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation\",\"lang\":\"en-US\",\"frontmatter\":{\"title\":\"Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation\",\"author\":\"Zichen Chen\",\"date\":\"2021-11-30T00:00:00.000Z\",\"tag\":[\"Speech Translation\",\"BERT\"],\"category\":[\"ST\",\"DL4MT\"],\"summary\":\"Speech translation (ST) has increasing demand in our daily life and work. Applications like travel assistant, simultaneous conference translation and movie subtitling can highly reduce translation costs. Building a ST system that can understand and directly translate acoustic speech signals into text in a target language is challenging. For example, people do not always premeditate what they are going to say. Not like text translation, ST lacks completed organization sometimes. Another part is that the parallel corpus for ST is not enough, compared to the MT task. Especially, most ST methods are limited by the amount of parallel corpus.\\n\",\"head\":[[\"meta\",{\"property\":\"og:url\",\"content\":\"https://lileicc.github.io/blog/dl4mt/2021/lut/\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"MLNLP Blog\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:image\",\"content\":\"https://lileicc.github.io/blog/\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"en-US\"}],[\"meta\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"meta\",{\"name\":\"twitter:image:alt\",\"content\":\"Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation\"}],[\"meta\",{\"property\":\"article:author\",\"content\":\"Zichen Chen\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"Speech Translation\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"BERT\"}],[\"meta\",{\"property\":\"article:published_time\",\"content\":\"2021-11-30T00:00:00.000Z\"}]]},\"excerpt\":\"<p>Speech translation (ST) has increasing demand in our daily life and work. Applications like travel assistant, simultaneous conference translation and movie subtitling can highly reduce translation costs. Building a ST system that can understand and directly translate acoustic speech signals into text in a target language is challenging. For example, people do not always premeditate what they are going to say. Not like text translation, ST lacks completed organization sometimes. Another part is that the parallel corpus for ST is not enough, compared to the MT task. Especially, most ST methods are limited by the amount of parallel corpus.</p>\\n\",\"headers\":[{\"level\":2,\"title\":\"End-to-end ST\",\"slug\":\"end-to-end-st\",\"link\":\"#end-to-end-st\",\"children\":[]},{\"level\":2,\"title\":\"Inspired by Human — LUT\",\"slug\":\"inspired-by-human-—-lut\",\"link\":\"#inspired-by-human-—-lut\",\"children\":[]},{\"level\":2,\"title\":\"Evaluation of the LUT\",\"slug\":\"evaluation-of-the-lut\",\"link\":\"#evaluation-of-the-lut\",\"children\":[{\"level\":3,\"title\":\"Semantic Analysis\",\"slug\":\"semantic-analysis\",\"link\":\"#semantic-analysis\",\"children\":[]},{\"level\":3,\"title\":\"LUT vs Cascaded Model\",\"slug\":\"lut-vs-cascaded-model\",\"link\":\"#lut-vs-cascaded-model\",\"children\":[]}]}],\"readingTime\":{\"minutes\":5,\"words\":1499},\"filePathRelative\":\"dl4mt/2021/lut/README.md\",\"localizedDate\":\"November 29, 2021\"}")

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
