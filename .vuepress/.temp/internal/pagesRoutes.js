export const pagesRoutes = [
  ["v-8daa1a0e","/",{"title":"Blog Home","icon":"home","type":"home","readingTime":{"minutes":0.1,"words":29},"excerpt":""},["/index.html","/README.md"]],
  ["v-037c250d","/nlp/LOREN/",{"title":"Automatic Verification of Natural Language Claims","type":"article","readingTime":{"minutes":8.37,"words":2512},"excerpt":"<p>How to develop a model to verify a natural language statement while explaining its rationale?</p>\n<p>Reading Time: About 10 minutes.</p>\n","author":"Ziyue Wang","date":"2022-04-08T00:00:00.000Z","localizedDate":"April 7, 2022","category":["NLP"],"tag":["Fact Verafication","Reasoning","Logic-regularized neural network"]},["/nlp/LOREN/index.html","/nlp/LOREN/README.md"]],
  ["v-0060584c","/nlp/GAIN/",{"title":"Document-level Relation Extraction","type":"article","readingTime":{"minutes":5.22,"words":1566},"excerpt":"<p>The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications. Previous methods focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, <strong>extracting relations at the document-level is necessary</strong> for a holistic understanding of knowledge in text.\nThis blog describes a recent work on document-level relation extraction by Zeng et al. EMNLP 2020.</p>\n","author":"Runxin Xu","date":"2020-11-20T00:00:00.000Z","localizedDate":"November 19, 2020","category":["IE"],"tag":["Relation Extraction"]},["/nlp/GAIN/index.html","/nlp/GAIN/README.md"]],
  ["v-291e7daf","/nlp/bert-flow/",{"title":"What is the problem with BERT embeddings and how to fix them?","type":"article","readingTime":{"minutes":3.55,"words":1066},"excerpt":"<p>This blog presents an easy fix to the sentence embeddings learned by pre-trained language models.\nIt is based on the paper: On the Sentence Embeddings from Pre-trained Language Models by Li et al EMNLP 2020.</p>\n","author":"Bohan Li","date":"2020-11-04T00:00:00.000Z","localizedDate":"November 3, 2020","category":["NLP"],"tag":["Pre-training","BERT","Embedding"]},["/nlp/bert-flow/index.html","/nlp/bert-flow/README.md"]],
  ["v-af4d51b6","/mt/mrasp/",{"title":"Multilingual MT Pre-training --- mRASP","type":"article","readingTime":{"minutes":12.62,"words":3786},"excerpt":"<p>​\tIn 1920, the great philosopher Bertrand Russell visited China, accompanied by Yuen Ren Chao, a Chinese-American linguist. Mr. Chao was a naturally gifted polyglot. At that time, he could already speak Baoding dialect, Wu dialect, Fuzhou dialect, Nanjing dialect, and English. He accompanied Russell from Shanghai to Changsha by ship. During the trip, he was learning Changsha dialect from Yang Ruiliu, an economist on the same ship. When the ship docked in Changsha, Yuen Ren Chao was already able to translate Russell's speeches and slang into Changsha dialect. Can our neural network  become a model like &quot;Yuen Ren Chao&quot; on machine translation? That is, to create a unified model with multilingual abilities, and when encountering new languages, the model could quickly adapt to translating new ones after training with a small amount of data.</p>\n","author":"Xiao Pan","date":"2020-12-31T00:00:00.000Z","localizedDate":"December 30, 2020","category":["MT"],"tag":["Multilingual MT","Pre-training"]},["/mt/mrasp/index.html","/mt/mrasp/README.md"]],
  ["v-5ef099df","/mt/VOLT/",{"title":"Learning Optimal Vocabularies for Machine Translation in 30 Minutes","type":"article","readingTime":{"minutes":8.78,"words":2633},"excerpt":"<p>Constructing a vocabulary is a fisrt step for any NLP tasks.\nHow can we efficiently learn an optimal vocabulary for machine translation?\nIn this blog, I will explain the VOLT algorithm from the paper <em>Vocabulary Leaning via Optimal Transport for Neural Machine Translation</em>, which was awarded the best paper at ACL 2021.</p>\n<p>Reading time: About 8 minutes</p>\n","author":"Ahmed Elkordy","date":"2022-05-17T00:00:00.000Z","localizedDate":"May 16, 2022","category":["MT"],"tag":["Multilingual MT","Vocabulary Learning","Optimal Transport"],"star":true},["/mt/VOLT/index.html","/mt/VOLT/README.md"]],
  ["v-d12a3a8c","/mt/ctnmt/",{"title":"机器翻译中的 BERT 应用","type":"article","readingTime":{"minutes":6.22,"words":1866},"excerpt":"<p>​\t预训练技术，比如 BERT等，在自然语言处理领域，尤其是自然语言理解任务取得了巨大的成功。然而目前预训练技术在文本生成领域，比如机器翻译领域，能够取得什么样的效果，还是一个开放问题。CTNMT 这篇论文，从三个方面介绍这个问题：</p>\n<ol>\n<li>预训练技术，比如 BERT或者 GPT 在机器翻译中的应用存在什么挑战？</li>\n<li>针对这些调整，需要怎么最大程度利用预训练知识？</li>\n<li>预训练和机器翻译的融合还有什么潜力？</li>\n</ol>\n","author":"王明轩","date":"2020-12-31T00:00:00.000Z","localizedDate":"December 30, 2020","category":["MT"],"tag":["Machine Translation","BERT","Pre-training"]},["/mt/ctnmt/index.html","/mt/ctnmt/README.md"]],
  ["v-75a8c0a1","/dl4mt/2021/chimera/",{"title":"Learning Shared Semantic Space for Speech-to-Text Translation","type":"article","readingTime":{"minutes":9.65,"words":2896},"excerpt":"<p>How to develop a translation model that can take both speech and text as input and translate to target language?\nCan we borrow inspiration from human brain study to improve the speech translation models?</p>\n<p>Reading Time: About 15 minutes.</p>\n","author":"Xianjun Yang","date":"2021-11-13T00:00:00.000Z","localizedDate":"November 12, 2021","category":["ST","DL4MT"],"tag":["Speech Translation","Shared Semantic Memory","Chimera"]},["/dl4mt/2021/chimera/index.html","/dl4mt/2021/chimera/README.md"]],
  ["v-a6614676","/dl4mt/2021/imigit/",{"title":"Can Visual Imagination Help Machine Translation?","type":"article","readingTime":{"minutes":4.07,"words":1222},"excerpt":"<p>Machine translation has helped people daily life, and is also an important research topic especially in computer science community.\nIt expands from one language translate to another language, speech translate to text, etc.\nToday, I'm going to talk about a paper &quot;Generative Imagination Elevates Machine Translation&quot;. I'll cover the background, challenge and motivation behind this paper. Then I'll go through some technical details of this paper as well as some in-depth analysis of their experimental settings and results. Finally, we will discuss about the potential extension of this work. Hopefully, this would give you a better understanding of this area, and point out to a promising research direction.</p>\n","author":"Yujie Lu","date":"2021-11-21T00:00:00.000Z","localizedDate":"November 20, 2021","category":["MT","DL4MT"],"tag":["Imagination","Visual Machine Translation","ImagiT"]},["/dl4mt/2021/imigit/index.html","/dl4mt/2021/imigit/README.md"]],
  ["v-4e089efc","/dl4mt/2021/comet/",{"title":"Automatic Machine Translation Evaluation - COMET Explained","type":"article","readingTime":{"minutes":2.61,"words":783},"excerpt":"Motivation While the advance in deep learning has dramatically improved the machine translation quality, there is little development in the evaluation of machine translation models","author":"Xinyi Wang","date":"2021-12-01T00:00:00.000Z","localizedDate":"November 30, 2021","category":["MT","DL4MT"],"tag":["MT Evaluation","Pre-training"]},["/dl4mt/2021/comet/index.html","/dl4mt/2021/comet/README.md"]],
  ["v-4e141a4b","/dl4mt/2021/cpgmt/",{"title":"Contextual Parameter Generation for Universal Neural Machine Translation","type":"article","readingTime":{"minutes":6.52,"words":1955},"excerpt":"Introduction A typical neural machine translation (NMT) system needs to support the translation among various languages, that is, a multilingual(many-to-many) NMT system rather tha","author":"Bairu Hou","date":"2021-11-28T00:00:00.000Z","localizedDate":"November 27, 2021","category":["MT","DL4MT"],"tag":["Multilingual MT"]},["/dl4mt/2021/cpgmt/index.html","/dl4mt/2021/cpgmt/README.md"]],
  ["v-60c9a2e1","/dl4mt/2021/gpt/",{"title":"Generative Pre-trained Transformer (GPT)","type":"article","readingTime":{"minutes":8.21,"words":2462},"excerpt":"<p>In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine translation systems, and alleviated the need for Recurrent Neural Networks and LSTMs to derive context and meaning for sequence to sequence modeling. Since the <em> Attention Is All You Need </em> paper was published in 2017, there have been many experimental application and fine-tuning improvements made upon the original model. The latest such improvement is the Generative Pre-Trained Transformer 3, or GPT-3.</p>\n","author":"Alex Rasla","date":"2021-11-01T00:00:00.000Z","localizedDate":"October 31, 2021","category":["NLG","DL4MT"],"tag":["Pre-training","Language Modelling"]},["/dl4mt/2021/gpt/index.html","/dl4mt/2021/gpt/README.md"]],
  ["v-6c37a1e9","/dl4mt/2021/lightseq/",{"title":"Accelerating the Computation on GPUs for Natural Language Processing","type":"article","readingTime":{"minutes":4.14,"words":1241},"excerpt":"<p>A high performance open-source library for NLP Transformer model training and inferencing.</p>\n","author":"Bowen Zhang","date":"2021-12-10T00:00:00.000Z","localizedDate":"December 9, 2021","category":["NLP","DL4MT"],"tag":["Transformer","GPU Acceleration","CUDA"]},["/dl4mt/2021/lightseq/index.html","/dl4mt/2021/lightseq/README.md"]],
  ["v-60cbfb81","/dl4mt/2021/lut/",{"title":"Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation","type":"article","readingTime":{"minutes":5,"words":1499},"excerpt":"<p>Speech translation (ST) has increasing demand in our daily life and work. Applications like travel assistant, simultaneous conference translation and movie subtitling can highly reduce translation costs. Building a ST system that can understand and directly translate acoustic speech signals into text in a target language is challenging. For example, people do not always premeditate what they are going to say. Not like text translation, ST lacks completed organization sometimes. Another part is that the parallel corpus for ST is not enough, compared to the MT task. Especially, most ST methods are limited by the amount of parallel corpus.</p>\n","author":"Zichen Chen","date":"2021-11-30T00:00:00.000Z","localizedDate":"November 29, 2021","category":["ST","DL4MT"],"tag":["Speech Translation","BERT"]},["/dl4mt/2021/lut/index.html","/dl4mt/2021/lut/README.md"]],
  ["v-8eab3c46","/dl4mt/2021/lass/",{"title":"Exploiting Capacity for Multilingual Neural Machine Translation","type":"article","readingTime":{"minutes":4.34,"words":1302},"excerpt":"<p>Multiligual machine translation aims at learning a single tanslation model for multiple languages. However, high resource language often suffers from performance degradation.\nIn this blog, we present a  method  LaSS proposed in a recent ACL paper on multilingual neural machine translation.\nThe LaSS is an approach to jointly train a single unified multilingual MT model and learns language-specific subnetwork for each language pair. Authors conducted experiments on IWSLT and WMT datasets with various Transformer architectures. The experimental results demonstrates average 1.2 BLEU improvements on 36 language pairs. LaSS shows strong generalization capabilty and demonstrates strong performance in zero-shot translation. Specifically, LaSS achieves 8.3 BLEU on 30 language pairs.</p>\n","author":"Wenda Xu","date":"2021-11-19T00:00:00.000Z","localizedDate":"November 18, 2021","category":["MT","DL4MT"],"tag":["Multilingual MT","Model Capacity","Language-specific Sub-network"]},["/dl4mt/2021/lass/index.html","/dl4mt/2021/lass/README.md"]],
  ["v-5ea8eb91","/dl4mt/2021/mgnmt/",{"title":"Mirror-Generative Neural Machine Translation","type":"article","readingTime":{"minutes":4.26,"words":1277},"excerpt":"<p>In general, neural machine translation (NMT) requires a large amount of parallel data (e.g., EN-&gt;CN). However, it is not easy to collect enough high-quality parallelly-paired sentences for training the translation model. On the other hand, we can capture enormous plain text from Wikipedia or news articles for each specific language. In this paper, MGNMT tries to make good use of non-parallel data and boost the performance of NMT.</p>\n","author":"Tsu-Jui Fu","date":"2021-11-25T00:00:00.000Z","localizedDate":"November 24, 2021","category":["MT","DL4MT"],"tag":["Variational Inference","Latent Variable Model","Semi-supervised Learning"]},["/dl4mt/2021/mgnmt/index.html","/dl4mt/2021/mgnmt/README.md"]],
  ["v-eef560ce","/dl4mt/2021/mrasp2/",{"title":"Contrastive Learning for Many-to-many Multilingual Neural Machine Translation","type":"article","readingTime":{"minutes":5.23,"words":1569},"excerpt":"<p>How to develop a single unified model to translate from any language to any language?\nThis work proposes a many-to-many translation system with emphasis on both English-centric and non-English directions. Many recent works have focused on proposing a single unified model for multiligual translation. These models are favorable because they are efficient and easy for deployment. However, most of these works focus on improving English-centric directions, which means that translation between two arbitrary languages may not be well supported. Therefore, in this paper, they propose a training method called mRASP2, including contrastive learning and alignment augmentation (AA) to train a unified multilingual translation system.  They also contribute a monolingual dataset called MC24. By making use of monolingual and bilingual language copora, the system is able to learn language-agnostic representation to support non-English directions better than before. Their system achieves great performances and outperforms a strong Transformer baseline by a large margin.</p>\n","author":"Weixi Feng","date":"2021-11-20T00:00:00.000Z","localizedDate":"November 19, 2021","category":["MT","DL4MT"],"tag":["Multilingual MT","Contrastive Learning","Zero-shot Translation","mRASP"],"star":true},["/dl4mt/2021/mrasp2/index.html","/dl4mt/2021/mrasp2/README.md"]],
  ["v-a1211dca","/dl4mt/2021/mtmetric/",{"title":"Learned Metrics for Machine Translation","type":"article","readingTime":{"minutes":12.69,"words":3808},"excerpt":"<p>How to automatically evaluate the quality of a machine translation system? Human evaluation is accurate, but expensive. It is not suitable for MT model development.</p>\n<p>Reading Time: About 15 minutes.</p>\n","author":"Huake He","date":"2021-11-01T00:00:00.000Z","localizedDate":"October 31, 2021","category":["MT","DL4MT"],"tag":["MT Evaluation","BERTScore","COMET","BERT"]},["/dl4mt/2021/mtmetric/index.html","/dl4mt/2021/mtmetric/README.md"]],
  ["v-e648c9de","/dl4mt/2021/self-training/",{"title":"Revisiting Self-training for Neural Sequence Generation","type":"article","readingTime":{"minutes":4.74,"words":1423},"excerpt":"<p>Self-training is a very prevalent semi-supervised method. Its key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the <em>pseudo-parallel</em> data). Self-training has been widely used in classification tasks. However, will it work on sequence generation tasks (e.g. machine translation)? If so, how does it work? This blog introduces a work [1] which investigates these questions and gives the answers.</p>\n","author":"Zekun Li","date":"2021-12-05T00:00:00.000Z","localizedDate":"December 4, 2021","category":["MT","DL4MT"],"tag":["Self-training"]},["/dl4mt/2021/self-training/index.html","/dl4mt/2021/self-training/README.md"]],
  ["v-3d2c52a4","/dl4mt/2021/unsupervisedmt/",{"title":"Unsupervised Machine Translation using Monolingual Corpora Only","type":"article","readingTime":{"minutes":5.63,"words":1688},"excerpt":"<p>Can one build a neural machine translation model without parallel data?</p>\n","author":"Qiucheng Wu","date":"2021-11-28T00:00:00.000Z","localizedDate":"November 27, 2021","category":["MT","DL4MT"],"tag":["Unsupervised Machine Translation"]},["/dl4mt/2021/unsupervisedmt/index.html","/dl4mt/2021/unsupervisedmt/README.md"]],
  ["v-7cb553dd","/dl4mt/2021/nmt-translation-memory/",{"title":"Neural Machine Translation with Monolingual Translation Memory","type":"article","readingTime":{"minutes":8.04,"words":2411},"excerpt":"Hello fellow readers! In this post, I would like to share a recent advance in the field of Machine Translation. Specifically, I will be presenting the paper Neural Machine Translat","author":"Rajan Saini","date":"2021-12-08T00:00:00.000Z","localizedDate":"December 7, 2021","category":["MT","DL4MT"],"tag":["Translation Memory"]},["/dl4mt/2021/nmt-translation-memory/index.html","/dl4mt/2021/nmt-translation-memory/README.md"]],
  ["v-5b1d4715","/dl4mt/2021/recurrent-attention/",{"title":"Recurrent Attention for Neural Machine Translation","type":"article","readingTime":{"minutes":5.23,"words":1570},"excerpt":"<p>​Upon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?</p>\n","author":"Jiachen Li","date":"2021-11-29T00:00:00.000Z","localizedDate":"November 28, 2021","category":["MT","DL4MT"],"tag":["Transformer","Recurrent Attention"]},["/dl4mt/2021/recurrent-attention/index.html","/dl4mt/2021/recurrent-attention/README.md"]],
  ["v-3706649a","/404.html",{"title":"","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/404"]],
  ["v-5bc93818","/category/",{"title":"Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/index.html"]],
  ["v-744d024e","/tag/",{"title":"Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/index.html"]],
  ["v-e52c881c","/article/",{"title":"Articles","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/article/index.html"]],
  ["v-75ed4ea4","/encrypted/",{"title":"Encrypted","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/encrypted/index.html"]],
  ["v-d804e652","/slide/",{"title":"Slides","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/slide/index.html"]],
  ["v-154dc4c4","/star/",{"title":"Star","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/star/index.html"]],
  ["v-01560935","/timeline/",{"title":"Timeline","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/timeline/index.html"]],
  ["v-65f31015","/category/nlp/",{"title":"NLP Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/nlp/index.html"]],
  ["v-09f249a9","/tag/fact-verafication/",{"title":"Fact Verafication Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/fact-verafication/index.html"]],
  ["v-3d18477b","/category/ie/",{"title":"IE Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/ie/index.html"]],
  ["v-eb85a4c2","/tag/reasoning/",{"title":"Reasoning Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/reasoning/index.html"]],
  ["v-3d185850","/category/mt/",{"title":"MT Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/mt/index.html"]],
  ["v-452400f1","/tag/logic-regularized-neural-network/",{"title":"Logic-regularized neural network Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/logic-regularized-neural-network/index.html"]],
  ["v-3d186ed6","/category/st/",{"title":"ST Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/st/index.html"]],
  ["v-5ffdde25","/tag/relation-extraction/",{"title":"Relation Extraction Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/relation-extraction/index.html"]],
  ["v-b776cd58","/category/dl4mt/",{"title":"DL4MT Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/dl4mt/index.html"]],
  ["v-2d06b746","/tag/pre-training/",{"title":"Pre-training Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/pre-training/index.html"]],
  ["v-65f30efe","/category/nlg/",{"title":"NLG Category","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/category/nlg/index.html"]],
  ["v-2832e17c","/tag/bert/",{"title":"BERT Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/bert/index.html"]],
  ["v-349d9c86","/tag/embedding/",{"title":"Embedding Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/embedding/index.html"]],
  ["v-1f5d423a","/tag/multilingual-mt/",{"title":"Multilingual MT Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/multilingual-mt/index.html"]],
  ["v-09bf8bf8","/tag/vocabulary-learning/",{"title":"Vocabulary Learning Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/vocabulary-learning/index.html"]],
  ["v-0039021f","/tag/optimal-transport/",{"title":"Optimal Transport Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/optimal-transport/index.html"]],
  ["v-e661029c","/tag/machine-translation/",{"title":"Machine Translation Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/machine-translation/index.html"]],
  ["v-8f24e4ca","/tag/speech-translation/",{"title":"Speech Translation Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/speech-translation/index.html"]],
  ["v-218136be","/tag/shared-semantic-memory/",{"title":"Shared Semantic Memory Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/shared-semantic-memory/index.html"]],
  ["v-2004851c","/tag/chimera/",{"title":"Chimera Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/chimera/index.html"]],
  ["v-3c7b1652","/tag/imagination/",{"title":"Imagination Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/imagination/index.html"]],
  ["v-4dd154c3","/tag/visual-machine-translation/",{"title":"Visual Machine Translation Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/visual-machine-translation/index.html"]],
  ["v-65fe45cc","/tag/imagit/",{"title":"ImagiT Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/imagit/index.html"]],
  ["v-4c989f1b","/tag/mt-evaluation/",{"title":"MT Evaluation Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/mt-evaluation/index.html"]],
  ["v-57e1c5f7","/tag/language-modelling/",{"title":"Language Modelling Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/language-modelling/index.html"]],
  ["v-a7c358f8","/tag/transformer/",{"title":"Transformer Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/transformer/index.html"]],
  ["v-1310c3c0","/tag/gpu-acceleration/",{"title":"GPU Acceleration Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/gpu-acceleration/index.html"]],
  ["v-28480812","/tag/cuda/",{"title":"CUDA Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/cuda/index.html"]],
  ["v-502d9903","/tag/model-capacity/",{"title":"Model Capacity Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/model-capacity/index.html"]],
  ["v-3a6a4a82","/tag/language-specific-sub-network/",{"title":"Language-specific Sub-network Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/language-specific-sub-network/index.html"]],
  ["v-211606b3","/tag/variational-inference/",{"title":"Variational Inference Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/variational-inference/index.html"]],
  ["v-90942cf8","/tag/latent-variable-model/",{"title":"Latent Variable Model Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/latent-variable-model/index.html"]],
  ["v-1ebf42d9","/tag/semi-supervised-learning/",{"title":"Semi-supervised Learning Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/semi-supervised-learning/index.html"]],
  ["v-7b21b8cc","/tag/contrastive-learning/",{"title":"Contrastive Learning Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/contrastive-learning/index.html"]],
  ["v-51b66b7a","/tag/zero-shot-translation/",{"title":"Zero-shot Translation Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/zero-shot-translation/index.html"]],
  ["v-1cc3cf38","/tag/mrasp/",{"title":"mRASP Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/mrasp/index.html"]],
  ["v-66bb7ab0","/tag/bertscore/",{"title":"BERTScore Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/bertscore/index.html"]],
  ["v-3f2ec7e6","/tag/comet/",{"title":"COMET Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/comet/index.html"]],
  ["v-6b0c11bc","/tag/self-training/",{"title":"Self-training Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/self-training/index.html"]],
  ["v-70873db4","/tag/unsupervised-machine-translation/",{"title":"Unsupervised Machine Translation Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/unsupervised-machine-translation/index.html"]],
  ["v-46418724","/tag/translation-memory/",{"title":"Translation Memory Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/translation-memory/index.html"]],
  ["v-aa5edb34","/tag/recurrent-attention/",{"title":"Recurrent Attention Tag","type":"page","readingTime":{"minutes":0,"words":0},"excerpt":""},["/tag/recurrent-attention/index.html"]],
]
