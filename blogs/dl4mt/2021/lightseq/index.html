<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Accelerating the Computation on GPUs for Natural Language Processing | Learning and Language Blog</title>
    <meta name="generator" content="VuePress 1.9.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.b14e6ae4.css" as="style"><link rel="preload" href="/blog/assets/js/app.c1ef5d59.js" as="script"><link rel="preload" href="/blog/assets/js/3.c983ed68.js" as="script"><link rel="preload" href="/blog/assets/js/1.7afedd4b.js" as="script"><link rel="preload" href="/blog/assets/js/10.29eb5e24.js" as="script"><link rel="prefetch" href="/blog/assets/js/11.19b0a61e.js"><link rel="prefetch" href="/blog/assets/js/12.55b30af0.js"><link rel="prefetch" href="/blog/assets/js/13.c4e7f212.js"><link rel="prefetch" href="/blog/assets/js/14.3a631fd2.js"><link rel="prefetch" href="/blog/assets/js/15.62b670d7.js"><link rel="prefetch" href="/blog/assets/js/16.f6a6b79e.js"><link rel="prefetch" href="/blog/assets/js/17.139fca72.js"><link rel="prefetch" href="/blog/assets/js/18.893dd88b.js"><link rel="prefetch" href="/blog/assets/js/19.134ffbd0.js"><link rel="prefetch" href="/blog/assets/js/20.52155e08.js"><link rel="prefetch" href="/blog/assets/js/21.2883514b.js"><link rel="prefetch" href="/blog/assets/js/22.210f1ad7.js"><link rel="prefetch" href="/blog/assets/js/23.6aa81fab.js"><link rel="prefetch" href="/blog/assets/js/24.9b481dc1.js"><link rel="prefetch" href="/blog/assets/js/25.e87dd3a6.js"><link rel="prefetch" href="/blog/assets/js/26.eccc04e8.js"><link rel="prefetch" href="/blog/assets/js/27.ca4619f7.js"><link rel="prefetch" href="/blog/assets/js/28.db4c1ba4.js"><link rel="prefetch" href="/blog/assets/js/29.0b766ca8.js"><link rel="prefetch" href="/blog/assets/js/30.6622051c.js"><link rel="prefetch" href="/blog/assets/js/4.bd476539.js"><link rel="prefetch" href="/blog/assets/js/5.398608b0.js"><link rel="prefetch" href="/blog/assets/js/6.13041dc7.js"><link rel="prefetch" href="/blog/assets/js/7.09916035.js"><link rel="prefetch" href="/blog/assets/js/8.aca121cd.js"><link rel="prefetch" href="/blog/assets/js/9.0907e06a.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.b14e6ae4.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-130b300a><div data-v-130b300a><div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-130b300a data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Learning and Language Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-130b300a><header class="navbar" data-v-130b300a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Learning and Language Blog" class="logo"> <span class="site-name">Learning and Language Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-130b300a></div> <aside class="sidebar" data-v-130b300a><div class="personal-info-wrapper" data-v-39576ba9 data-v-130b300a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    Lei Li
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>20</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>37</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Accelerating the Computation on GPUs for Natural Language Processing</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-130b300a><main class="page"><section><div class="page-title"><h1 class="title">Accelerating the Computation on GPUs for Natural Language Processing</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Bowen Zhang</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>12/10/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Transformer</span><span class="tag-item" data-v-f875f3fc>GPU Acceleration</span><span class="tag-item" data-v-f875f3fc>CUDA</span><span class="tag-item" data-v-f875f3fc>High performance computing</span></i></div></div> <div class="theme-reco-content content__default"><p>A high performance open-source library for NLP Transformer model training and inferencing.
</p> <h2 id="_1-what-is-lightseq"><a href="#_1-what-is-lightseq" class="header-anchor">#</a> 1. What is LightSeq?</h2> <p><img src="/blog/assets/img/lightseqlogo.819d5933.png" alt="logo"></p> <h3 id="_1-1-nlp-models"><a href="#_1-1-nlp-models" class="header-anchor">#</a> 1.1 NLP models</h3> <p>Transformers[1], BERT[2], or GPT[3] models are state-of-art models on natural language processing tasks. They are heavily used and breaking multiple records on sequence-to-sequence (Seq2Seq) tasks including machine translation, text summarization, and text generation, or even computer vision tasks by Vision Transformers (an image is just a sequence of pixels). However, those models are huge in size that needs large-scale training and inference. This makes it computationally expensive, so serving these models is a challenge for real industrial applications.</p> <h3 id="_1-2-motivation"><a href="#_1-2-motivation" class="header-anchor">#</a> 1.2 Motivation</h3> <p>Due to the high complexity and large parameter size of transformer models, the latency for both training and inference is high. Here are the three comparisons between the current inference systems and LightSeq, and the reasons why they are not able to perform well for online tasks.</p> <ol><li>Popular deep learning frameworks. Since those models have flexible model structures, both TensorFlow and PyTorch need additional memory allocation and extra overhead for training. Thus, they do not make full use of the hardware resource.</li> <li>Inference optimization frameworks. Optimization frameworks like TensorFlow XLA, TVM, and TensorRT are not suitable for variable-length inputs, which require dynamic memory allocation that does not perform well for transformer models.</li> <li>Similar acceleration frameworks. Faster-Transformer and TurboTransformers are similar to LightSeq. However, they do not have all components or features compared to LightSeq (Table 1).</li></ol> <h3 id="_1-3-lightseq"><a href="#_1-3-lightseq" class="header-anchor">#</a> 1.3 Lightseq</h3> <p>LightSeq[4], is a high-performance open-source library for both training and inference that is directly built on top of CUDA official libraries (<a href="https://docs.nvidia.com/cuda/cublas/index.html" target="_blank" rel="noopener noreferrer">cuBLAS<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <a href="https://docs.nvidia.com/cuda/thrust/index.html" target="_blank" rel="noopener noreferrer">Thrust<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <a href="http://nvlabs.github.io/cub/" target="_blank" rel="noopener noreferrer">CUB<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>).
It supports models in the Transformer family including BERT, GPT, and full encoder-decoder.
It introduces new transformer encoders and decoders components after fusing and optimizing the existing models.</p> <p>The applications of LightSeq include Machine Translation, Text Generation, Dialog, Language Modelling, Sentiment Analysis, and other related tasks with sequence data, which can be easily deployed to commercial products.</p> <p>LightSeq improves the speed for both training and inference stages. Models like DeepSpeed[5] only accelerate the training, and tools like TensorRT,  FasterTransformer, or TurboTransformers only support optimizing the inference. Here are the comparison tables on different features between LightSeq and other models.</p> <p><img src="/blog/assets/img/feature_table.42ca132e.png" alt="Features"></p> <p>Table 1. The tables above are from the <a href="https://github.com/bytedance/lightseq" target="_blank" rel="noopener noreferrer">official Github repository<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h2 id="_2-technique-details"><a href="#_2-technique-details" class="header-anchor">#</a> 2. Technique Details</h2> <p>There are three main methods that LightSeq uses to optimize the model, training speed, and inference speed. The image below shows the architecture of a sequence-to-sequence model using transformers.</p> <p><img src="/blog/assets/img/Transformer.40076700.png" alt="Transformer"></p> <h3 id="_2-1-operation-fusion"><a href="#_2-1-operation-fusion" class="header-anchor">#</a> 2.1 Operation Fusion</h3> <p>Transformers model implemented by popular deep learning frameworks like Pytorch or Tensorflow just combine multiple fine-grained kernel functions for one layer. In this way, it needs to launch more kernel functions and uses lots of memory I/O that costs extra time for training and inference.</p> <p>LightSeq uses general matrix multiply (GEMM) and custom kernel functions, so here are only six custom kernel functions and six GEMM in a Transformer encoder layer for LightSeq models. The right image shows the model structure of the LightSeq transformer encoder layer.</p> <p><em>Need to add more intuitive description here</em> <img src="/blog/assets/img/fusion.de07cf52.png" alt="Fusion"></p> <h3 id="_2-2-hierarchical-auto-regressive-search"><a href="#_2-2-hierarchical-auto-regressive-search" class="header-anchor">#</a> 2.2 Hierarchical Auto-Regressive Search</h3> <p>Searching usually happens in the last step of a transformer model. Redundant calculations often exist in output layers since we only need a few labels/tokens with the highest probability instead of all of them.</p> <p>LightSeq optimizes this process by using a Hierarchical Auto Regressive Search method to erase redundant calculations and perform parallel computing illustrated as below (using beam search as an example).</p> <p>the following steps happen for each beam.</p> <ol><li>Randomly divide logits into k groups</li> <li>Calculate the maximum of group <mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" viewBox="0 -661 345 672" style="vertical-align:-0.025ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>, denoted
as <mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="2.651ex" height="1.357ex" viewBox="0 -442 1172 599.8" style="vertical-align:-0.357ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6D" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></li> <li>Calculate the minimum of <mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="2.651ex" height="1.357ex" viewBox="0 -442 1172 599.8" style="vertical-align:-0.357ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6D" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>, denoted as <mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.593ex" viewBox="0 -683 759 704" style="vertical-align:-0.048ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="52" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g></g></svg></mjx-container>,
which can be regarded as a rough top-k value of logits.</li> <li>Select logits larger than <mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.593ex" viewBox="0 -683 759 704" style="vertical-align:-0.048ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="52" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g></g></svg></mjx-container> and write them into GPU memory.</li></ol> <p><img src="/blog/assets/img/softmax.a3130038.png" alt="softmax"></p> <h3 id="_2-3-dynamic-gpu-memory-reuse"><a href="#_2-3-dynamic-gpu-memory-reuse" class="header-anchor">#</a> 2.3 Dynamic GPU Memory Reuse</h3> <p>LightSeq pre-defines the maximum of dynamic shapes, such as the maximal sequence length, to avoid memory allocation time and save GPU memory occupancy. Also, GPU memory is shared for non-dependent intermediate results to reduce the memory usage.</p> <p>By using LightSeq, users are able to 8 Transformer big models simultaneously on a NVIDIA Tesla T4 GPU.</p> <h2 id="_3-using-lightseq"><a href="#_3-using-lightseq" class="header-anchor">#</a> 3. Using LightSeq</h2> <p>Running LightSeq requires one or more GPUs.</p> <h3 id="_3-1-installation"><a href="#_3-1-installation" class="header-anchor">#</a> 3.1 Installation</h3> <p>LightSeq installation from PyPI only supports python 3.6 to 3.8 on Linux for now. Consider compiling from source if you have other environments.</p> <p><code>pip install lightseq fairseq sacremoses transformers</code></p> <h3 id="_3-2-training-examples-using-lightseq"><a href="#_3-2-training-examples-using-lightseq" class="header-anchor">#</a> 3.2 Training examples using LightSeq</h3> <p>Training a translation task on wmt14 en2de dataset by running the following command.</p> <p><code>sh examples/training/fairseq/ls_fairseq_wmt14en2de.sh</code></p> <p>If you want to run the training using FairSeq, run the following command.</p> <p><code>sh examples/training/fairseq/fairseq_wmt14en2de.sh</code></p> <h3 id="_3-3-inference-examples-using-lightseq"><a href="#_3-3-inference-examples-using-lightseq" class="header-anchor">#</a> 3.3 Inference examples Using LightSeq</h3> <p><code>pip install torch tensorflow transformers lightseq</code></p> <p><code>cd examples/inference/python</code></p> <p><code>python export/hf_bart_export.py</code></p> <p><code>python test/ls_bart.py</code></p> <p><a href="https://github.com/bytedance/lightseq/blob/master/docs/guide.md" target="_blank" rel="noopener noreferrer">Here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> is a guide on using LightSeq for training and inference.</p> <h2 id="_4-performance"><a href="#_4-performance" class="header-anchor">#</a> 4. Performance</h2> <h3 id="_4-1-training-performance"><a href="#_4-1-training-performance" class="header-anchor">#</a> 4.1 Training Performance</h3> <p>The plots below are the experiment results on WMT14 English to German translation tasks using Transformer-Big models. In all plots, FairSeq+LightSeq models are able to improve the performance to 3.5X maximum.</p> <p><img src="/blog/assets/img/training_speed.20825958.png" alt="Training"></p> <p>The image above is from the <a href="https://github.com/bytedance/lightseq" target="_blank" rel="noopener noreferrer">official Github repository<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h3 id="_4-2-inference-performance"><a href="#_4-2-inference-performance" class="header-anchor">#</a> 4.2 Inference Performance</h3> <p>Here are the inference results using LightSeq, TensorFlow, PyTorch, and FasterTransformer on neural machine translation using Transformer-base models with beam search methods.</p> <p><img src="/blog/assets/img/inference_speed.32cb359c.png" alt="Inference"></p> <p>The image above is from the <a href="https://github.com/bytedance/lightseq" target="_blank" rel="noopener noreferrer">official Github repository<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h3 id="_4-3-more-inference-performance-on-nvidia-p4-and-t4"><a href="#_4-3-more-inference-performance-on-nvidia-p4-and-t4" class="header-anchor">#</a> 4.3 More Inference Performance on Nvidia P4 and T4</h3> <p>The three images below are from the <a href="https://segmentfault.com/a/1190000038523998" target="_blank" rel="noopener noreferrer">Volctrans Blog on segmentfault.com<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>X-axes are the Batch size and sequence length pairs, and Y-axes are the acceleration rates.</p> <p><img src="/blog/assets/img/beamsearch.87c3c841.png" alt="Beam Search T4"></p> <p><img src="/blog/assets/img/beamsearchp4.a8927aa8.png" alt="Beam Search P4"></p> <p><img src="/blog/assets/img/samplingp4.541e46a0.png" alt="Sampling"></p> <h3 id="_4-4-real-world-cloud-computing-delay-test-on-gpt"><a href="#_4-4-real-world-cloud-computing-delay-test-on-gpt" class="header-anchor">#</a> 4.4 Real-world Cloud Computing Delay Test on GPT</h3> <p>This plot shows the performance of deploying a GPT model to cloud computing. At 11:00, the delay performance decreased from 360 ms to 80 ms when LightSeq is turned on.</p> <p><img src="/blog/assets/img/realworkload.2e0249b8.png" alt="Real Work Load"></p> <p>The image above is from the <a href="https://segmentfault.com/a/1190000038523998" target="_blank" rel="noopener noreferrer">Volctrans Blog on segmentfault.com<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h2 id="_5-reference"><a href="#_5-reference" class="header-anchor">#</a> 5. Reference</h2> <p>[1] Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems. 2017.</p> <p>[2] Devlin, Jacob, et al. &quot;Bert: Pre-training of deep bidirectional transformers for language understanding.&quot; <em>arXiv preprint arXiv:1810.04805</em> (2018).</p> <p>[3] Brown, Tom B., et al. &quot;Language models are few-shot learners.&quot; arXiv preprint arXiv:2005.14165 (2020).</p> <p>[4] Wang, Xiaohui, et al. &quot;LightSeq: A High Performance Inference Library for Transformers.&quot; arXiv preprint arXiv:2010.13887 (2020).</p> <p>[5] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. <a href="https://dl.acm.org/doi/10.1145/3394486.3406703" target="_blank" rel="noopener noreferrer">In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD '20, Tutorial)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">5/25/2022, 3:25:59 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_1-what-is-lightseq" class="sidebar-link reco-side-_1-what-is-lightseq" data-v-cb1513f6>1. What is LightSeq?</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_1-1-nlp-models" class="sidebar-link reco-side-_1-1-nlp-models" data-v-cb1513f6>1.1 NLP models</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_1-2-motivation" class="sidebar-link reco-side-_1-2-motivation" data-v-cb1513f6>1.2 Motivation</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_1-3-lightseq" class="sidebar-link reco-side-_1-3-lightseq" data-v-cb1513f6>1.3 Lightseq</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_2-technique-details" class="sidebar-link reco-side-_2-technique-details" data-v-cb1513f6>2. Technique Details</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_2-1-operation-fusion" class="sidebar-link reco-side-_2-1-operation-fusion" data-v-cb1513f6>2.1 Operation Fusion</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_2-2-hierarchical-auto-regressive-search" class="sidebar-link reco-side-_2-2-hierarchical-auto-regressive-search" data-v-cb1513f6>2.2 Hierarchical Auto-Regressive Search</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_2-3-dynamic-gpu-memory-reuse" class="sidebar-link reco-side-_2-3-dynamic-gpu-memory-reuse" data-v-cb1513f6>2.3 Dynamic GPU Memory Reuse</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_3-using-lightseq" class="sidebar-link reco-side-_3-using-lightseq" data-v-cb1513f6>3. Using LightSeq</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_3-1-installation" class="sidebar-link reco-side-_3-1-installation" data-v-cb1513f6>3.1 Installation</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_3-2-training-examples-using-lightseq" class="sidebar-link reco-side-_3-2-training-examples-using-lightseq" data-v-cb1513f6>3.2 Training examples using LightSeq</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_3-3-inference-examples-using-lightseq" class="sidebar-link reco-side-_3-3-inference-examples-using-lightseq" data-v-cb1513f6>3.3 Inference examples Using LightSeq</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_4-performance" class="sidebar-link reco-side-_4-performance" data-v-cb1513f6>4. Performance</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_4-1-training-performance" class="sidebar-link reco-side-_4-1-training-performance" data-v-cb1513f6>4.1 Training Performance</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_4-2-inference-performance" class="sidebar-link reco-side-_4-2-inference-performance" data-v-cb1513f6>4.2 Inference Performance</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_4-3-more-inference-performance-on-nvidia-p4-and-t4" class="sidebar-link reco-side-_4-3-more-inference-performance-on-nvidia-p4-and-t4" data-v-cb1513f6>4.3 More Inference Performance on Nvidia P4 and T4</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_4-4-real-world-cloud-computing-delay-test-on-gpt" class="sidebar-link reco-side-_4-4-real-world-cloud-computing-delay-test-on-gpt" data-v-cb1513f6>4.4 Real-world Cloud Computing Delay Test on GPT</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lightseq/#_5-reference" class="sidebar-link reco-side-_5-reference" data-v-cb1513f6>5. Reference</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.c1ef5d59.js" defer></script><script src="/blog/assets/js/3.c983ed68.js" defer></script><script src="/blog/assets/js/1.7afedd4b.js" defer></script><script src="/blog/assets/js/10.29eb5e24.js" defer></script>
  </body>
</html>
