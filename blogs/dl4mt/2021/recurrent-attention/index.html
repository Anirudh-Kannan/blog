<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Recurrent Attention for Neural Machine Translation | MLNLP Blog</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.0a66d5ca.css" as="style"><link rel="preload" href="/blog/assets/js/app.ddb5f13a.js" as="script"><link rel="preload" href="/blog/assets/js/3.21d28d4e.js" as="script"><link rel="preload" href="/blog/assets/js/1.6e4b099c.js" as="script"><link rel="preload" href="/blog/assets/js/8.94065b85.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.2c41eca3.js"><link rel="prefetch" href="/blog/assets/js/11.e0f4856d.js"><link rel="prefetch" href="/blog/assets/js/12.a7bf6826.js"><link rel="prefetch" href="/blog/assets/js/13.13df8733.js"><link rel="prefetch" href="/blog/assets/js/14.f37c6a31.js"><link rel="prefetch" href="/blog/assets/js/15.60e8be58.js"><link rel="prefetch" href="/blog/assets/js/16.f6189ed6.js"><link rel="prefetch" href="/blog/assets/js/17.78afe6a7.js"><link rel="prefetch" href="/blog/assets/js/18.c66b3ed4.js"><link rel="prefetch" href="/blog/assets/js/19.a8dce2a5.js"><link rel="prefetch" href="/blog/assets/js/20.5730cb6f.js"><link rel="prefetch" href="/blog/assets/js/21.4dacb3bb.js"><link rel="prefetch" href="/blog/assets/js/22.4dfcbd8c.js"><link rel="prefetch" href="/blog/assets/js/23.0c1dd783.js"><link rel="prefetch" href="/blog/assets/js/24.2e3eb003.js"><link rel="prefetch" href="/blog/assets/js/4.a3eac548.js"><link rel="prefetch" href="/blog/assets/js/5.5e714213.js"><link rel="prefetch" href="/blog/assets/js/6.e3b5f083.js"><link rel="prefetch" href="/blog/assets/js/7.7222e29c.js"><link rel="prefetch" href="/blog/assets/js/9.bb7c7b31.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.0a66d5ca.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>MLNLP Blog</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Lei Li</span>
            
          <span data-v-4e82dffc>2016 - </span>
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="MLNLP Blog" class="logo"> <span class="site-name">MLNLP Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    Lei Li
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>14</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>19</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Recurrent Attention for Neural Machine Translation</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Lei Li</span>
            
          <span data-v-4e82dffc>2016 - </span>
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Recurrent Attention for Neural Machine Translation</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>Jiachen Li</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>11/29/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>Transformer</span><span class="tag-item" data-v-1ff7123e>Recurrent Attention</span></i></div></div> <div class="theme-reco-content content__default"><p>​Upon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?</p> <p>Reading Time: About 10 minutes.</p> <p>Paper：<a href="https://aclanthology.org/2021.emnlp-main.258/" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.emnlp-main.258/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Github: <a href="https://github.com/lemon0830/RAN" target="_blank" rel="noopener noreferrer">https://github.com/lemon0830/RAN<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>A recent work that appeared at the 2021 Conference of Empirical Methods in Natural Language Processing, dives into analyzing the efficacy of the dot-product self-attention module. As recent research has shown that most attention heads only learn simple positional patterns, this paper steps further towards this line and propose a novel substitute mechanism for self-attention: Recurrent AtteNtion (RAN).</p> <p>The basic idea of RAN is to directly learn attention weights without any token-to-token interaction and perform layer-to-layer interaction. By performing a massive number of experiments on 10 machine translation tasks, this paper empirically proves that the RAN models are competitive and outperform their Transformer counterparts in certain scenarios, with fewer parameters and inference time. Specifically, applying RAN to the decoder of Transformer yields consistent improvements by about +0.5 BLEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation tasks.</p> <p>This blog post is organized in the following way: 1) Brief introduction of the Transformer Neural Network, with a focus on the basics of the multi-head self-attention module 2) Problem associated with the self-attention module 3) The solution provided by the RAN mechanism 4) The performance and analysis of RAN.</p> <h2 id="multi-head-attention-module"><a href="#multi-head-attention-module" class="header-anchor">#</a> Multi-head Attention Module</h2> <p><img src="/blog/assets/img/transformer.a2f6c3ce.png" alt="image1"></p> <p>The figure above gives an overview of the Transformer Architecture. The left-hand side provides the encoder architecture, while the right-hand side gives the decoder architecture. Both of the encoder and decoder are stacked by N sub-layers, and the multi-head attention module is the main component in both the encoder and decoder layer. The encoder encodes the inputs and generates the context vector, which serves as an input to the decoder for decoding the output sequences. We refer the interested readers to the original paper [1] and only focus on the Multi-Head Attention module in this article.</p> <p><img src="/blog/assets/img/multi_head_attention.382a56b6.png" alt="image2"></p> <p>The figure above depicts the computation of the dot-product self-attention of the <code>k</code>-th head in the <code>l</code>-th encoder layer. Given a sequence of token representations with a length of <code>n</code>, the self-attention model first converts the representations into three matrice <code>Q</code>, <code>K</code> and <code>V</code>, representing queries, keys, and values, respectively. And <code>d_k</code> is the dimensionality of the vector in the <code>k</code>-th head. Then, the attention matrix is calculated via the dot product of queries and keys followed by rescaling:</p> <p><img src="/blog/assets/img/dot_product.ff623939.png" alt="image3"></p> <p>Finally, a softmax operation is applied on this unnormalized attention matrix, and then the output is used to compute a weighted sum of values:</p> <p><img src="/blog/assets/img/eq_2.f48ebaf5.png" alt="image4"></p> <p>where <code>H</code> is a new contextual representation of the <code>l</code>-th layer. This procedure can be implemented with a multi-head mechanism by projecting the input into different subspaces, which requires extra splitting and concatenation operations. The output is fed into a position-wise feed-forward network to get the final representations of this layer.</p> <h2 id="problem-associated-with-the-self-attention"><a href="#problem-associated-with-the-self-attention" class="header-anchor">#</a> Problem Associated with the Self-attention</h2> <p>While flexible, it has been proven that there exists redundant information with pair-wise calculation. Many studies have shown that pairwise self-attention is over-parameterized, leading to a costly inference [2, 3, 4]. The RAN method takes this direction to an extreme by showing that self-attention is empirically replaceable. And next, we will formally introduce the RAN method.</p> <h2 id="ran-recurrent-attention"><a href="#ran-recurrent-attention" class="header-anchor">#</a> RAN: Recurrent Attention</h2> <p>RAN consists of a set of global <code>Initial Attention Matrices</code> and a <code>Recurrent Transition Module</code>. Instead of computing the attention weights on the fly as in the original multi-head attention module in each layer, RAN directly learn the attention weights, denoted as</p> <p><img src="/blog/assets/img/initial_weight_matrix.4c48145f.png" alt="image6"></p> <p>which are exactly the so-called <code>Initial Attention Matrices</code>. Here <code>h</code> denotes the number of heads. On the other hand, the <code>Recurrent Transition Module</code> takes the set of <code>A0</code> as input, and recursively updates the attention matrices layer by layer. Note that the <code>Initial Attention Matrices</code>, the <code>Recurrent Transition Module</code>, and the other modules are optimized jointly. The attention matrices are completely agnostic to the input representations and can be retrieved directly without recomputation during inference.</p> <p><img src="/blog/assets/img/ran.7d0605a8.png" alt="image7"></p> <p>Figure above gives the model architecture of the RAN, where the dotted line denotes parameter sharing. It also shows the computation of the <code>k</code>-th head in the <code>l</code>-th encoder layer. The recurrent transition module obtains the attention weights in <code>l</code>-th layer <code>Rec(∗)</code> with the attention matrix from the last layer.</p> <p>Moreover, the <code>Recurrent Transition Module</code> is implemented using a single feed-forward network with tanh as its activation function followed by a layer normalization and a residual connection:</p> <p><img src="/blog/assets/img/recurrent_transition.bcecaffa.png" alt="image8"></p> <p>Notably, the parameters of the transition module are shared across all heads and all layers.</p> <h2 id="effectiveness-and-analysis-of-the-ran"><a href="#effectiveness-and-analysis-of-the-ran" class="header-anchor">#</a> Effectiveness and Analysis of the RAN.</h2> <h3 id="_1-main-results"><a href="#_1-main-results" class="header-anchor">#</a> 1. Main results</h3> <p>The original paper evaluates RAN on WMT and NIST translation tasks, including 10 different language pairs altogether. Besides, the authors tried to apply RAN to the encoder (RAN-E), the decoder (RAN-D), or both of them (RAN-ALL), respectively. They compare against the standard Transformer (TransF) [1], and the two most related works are Hard-coded Transformer (HCSA) [5] and Random Synthesizer. (Syn-R) [6].</p> <p>Table 1 shows the overall results on the ten language pairs. Compared with TransF, the RAN models consistently yield competitive or even better results against TransF on all datasets. Concretely, 0.13/0.16, 0.48/0.44, and 0.16/0.22 more average BLEU/SacreBLEU are achieved by RAN-E, RAND, and RAN-ALL, respectively. Although different languages have different linguistic and syntactic structures, RAN can learn reasonable global attention patterns over the whole training corpus.</p> <p><img src="/blog/assets/img/main_result.3dc00939.png" alt="image9"></p> <p>Interestingly, RAN-D performs best, which significantly outperforms the TransF on most language pairs. The biggest performance gain comes from the low resource translation task Tr⇒En where RAN-D outperforms TransF by 0.97/1.0 BLEU/SacreBLEU points. We conjecture that the position-based attention without tokenwise interaction is easier to learn and the RAN can capture more generalized attention patterns. By contrast, the dot-product self-attention is forced to learn the semantic relationship between tokens and may fall into sub-optimal local minima, especially when the training scale is low.  In brief, the improvement indicates that NMT systems can benefit from simplified decoders when training data is insufficient. Besides, although both RAN-E and RAN-D are effective, their effects can not be accumulated.</p> <p>Moreover, we can see that RAN-ALL vastly outperforms the other two related methods. RAN bridges the performance gap between Transformer and the models without the dot-product self-attention, demonstrating the effectiveness of RAN. And from the figure below, we can see that RAN-ALL successfully speeds up the inference phase.</p> <p><img src="/blog/assets/img/speed_up.cbd11bd5.png" alt="image10"></p> <h3 id="_2-analysis"><a href="#_2-analysis" class="header-anchor">#</a> 2. Analysis</h3> <p>The figure below visualizes the attention patterns of RAN over positions</p> <p><img src="/blog/assets/img/visual_attention.6f03cd58.png" alt="image11"></p> <p>We find that in the encoder, RAN focuses its attention on a local neighborhood around each position. Specifically, in the last
layer of the encoder, the weights become more concentrated, potentially due to the hidden representations being contextualized. Interestingly, except attending local windows to the current position, the decoder weights are most concentrated in the first token of target sequences. This may demonstrate the mechanism of decoder self-attention that the RAN decoder attends to source-side hidden states based on global source sentence representations aggregated by the start tokens.</p> <p><img src="/blog/assets/img/visual_layer.bb844d0f.png" alt="image12"></p> <p>The figure above depicts the Jensen-Shannon divergence of attention between each pair of layers. The conclusions are as follows: First, the attention similarity in TransF is not salient, but the attention distribution of adjacent layers is similar to some extent. Second, there are no noticeable patterns found in Syn-R. Third, as for RAN-ALL, the attention similarity is high, especially in the decoder (the JS-divergence ranges from 0.08 to 0.2), and is remarkable between adjacent layers.</p> <h2 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h2> <p>The RAN architecture is proposed to simplify the Transformer architecture for Neural Machine Translation without costly dot-product self-attention. It takes the <code>Initial Attention Matrices</code> as a whole and updates it by a <code>Recurrent Transition Module recurrently</code>. Experiments on ten representative translation tasks show the effectiveness of RAN.</p> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p>[1] Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems. 2017.</p> <p>[2] Sanh, Victor, et al. &quot;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.&quot; arXiv preprint arXiv:1910.01108 (2019).</p> <p>[3] Correia, Gonçalo M., et al. &quot;Adaptively sparse transformers.&quot; arXiv preprint arXiv:1909.00015 (2019).</p> <p>[4] Xiao, Tong, et al. Sharing attention weights for fast transformer. In Proceedings of IJCAI 2019, pages 5292–5298.</p> <p>[5] You, Weiqiu, et al. Hard-coded gaussian attention for neural machine translation. In Proceedings of ACL 2020, pages 7689–7700.</p> <p>[6] Tay, Yi, et al. &quot;Synthesizer: Rethinking self-attention for transformer models.&quot; International Conference on Machine Learning. PMLR, 2021.</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">12/6/2021, 11:02:49 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#introduction" class="sidebar-link reco-side-introduction" data-v-70334359>Introduction</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#multi-head-attention-module" class="sidebar-link reco-side-multi-head-attention-module" data-v-70334359>Multi-head Attention Module</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#problem-associated-with-the-self-attention" class="sidebar-link reco-side-problem-associated-with-the-self-attention" data-v-70334359>Problem Associated with the Self-attention</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#ran-recurrent-attention" class="sidebar-link reco-side-ran-recurrent-attention" data-v-70334359>RAN: Recurrent Attention</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#effectiveness-and-analysis-of-the-ran" class="sidebar-link reco-side-effectiveness-and-analysis-of-the-ran" data-v-70334359>Effectiveness and Analysis of the RAN.</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#_1-main-results" class="sidebar-link reco-side-_1-main-results" data-v-70334359>1. Main results</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#_2-analysis" class="sidebar-link reco-side-_2-analysis" data-v-70334359>2. Analysis</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#summary" class="sidebar-link reco-side-summary" data-v-70334359>Summary</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/recurrent-attention/#references" class="sidebar-link reco-side-references" data-v-70334359>References</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.ddb5f13a.js" defer></script><script src="/blog/assets/js/3.21d28d4e.js" defer></script><script src="/blog/assets/js/1.6e4b099c.js" defer></script><script src="/blog/assets/js/8.94065b85.js" defer></script>
  </body>
</html>
