<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Generative Pre-trained Transformer (GPT) | Learning and Language Blog</title>
    <meta name="generator" content="VuePress 1.9.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.b14e6ae4.css" as="style"><link rel="preload" href="/blog/assets/js/app.c1ef5d59.js" as="script"><link rel="preload" href="/blog/assets/js/3.c983ed68.js" as="script"><link rel="preload" href="/blog/assets/js/1.7afedd4b.js" as="script"><link rel="preload" href="/blog/assets/js/29.0b766ca8.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.29eb5e24.js"><link rel="prefetch" href="/blog/assets/js/11.19b0a61e.js"><link rel="prefetch" href="/blog/assets/js/12.55b30af0.js"><link rel="prefetch" href="/blog/assets/js/13.c4e7f212.js"><link rel="prefetch" href="/blog/assets/js/14.3a631fd2.js"><link rel="prefetch" href="/blog/assets/js/15.62b670d7.js"><link rel="prefetch" href="/blog/assets/js/16.f6a6b79e.js"><link rel="prefetch" href="/blog/assets/js/17.139fca72.js"><link rel="prefetch" href="/blog/assets/js/18.893dd88b.js"><link rel="prefetch" href="/blog/assets/js/19.134ffbd0.js"><link rel="prefetch" href="/blog/assets/js/20.52155e08.js"><link rel="prefetch" href="/blog/assets/js/21.2883514b.js"><link rel="prefetch" href="/blog/assets/js/22.210f1ad7.js"><link rel="prefetch" href="/blog/assets/js/23.6aa81fab.js"><link rel="prefetch" href="/blog/assets/js/24.9b481dc1.js"><link rel="prefetch" href="/blog/assets/js/25.e87dd3a6.js"><link rel="prefetch" href="/blog/assets/js/26.eccc04e8.js"><link rel="prefetch" href="/blog/assets/js/27.ca4619f7.js"><link rel="prefetch" href="/blog/assets/js/28.db4c1ba4.js"><link rel="prefetch" href="/blog/assets/js/30.6622051c.js"><link rel="prefetch" href="/blog/assets/js/4.bd476539.js"><link rel="prefetch" href="/blog/assets/js/5.398608b0.js"><link rel="prefetch" href="/blog/assets/js/6.13041dc7.js"><link rel="prefetch" href="/blog/assets/js/7.09916035.js"><link rel="prefetch" href="/blog/assets/js/8.aca121cd.js"><link rel="prefetch" href="/blog/assets/js/9.0907e06a.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.b14e6ae4.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-130b300a><div data-v-130b300a><div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-130b300a data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Learning and Language Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-130b300a><header class="navbar" data-v-130b300a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Learning and Language Blog" class="logo"> <span class="site-name">Learning and Language Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-130b300a></div> <aside class="sidebar" data-v-130b300a><div class="personal-info-wrapper" data-v-39576ba9 data-v-130b300a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    Lei Li
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>20</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>37</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Generative Pre-trained Transformer (GPT)</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-130b300a><main class="page"><section><div class="page-title"><h1 class="title">Generative Pre-trained Transformer (GPT)</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Alex Rasla</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/1/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Pre-trained Language Model</span><span class="tag-item" data-v-f875f3fc>GPT</span></i></div></div> <div class="theme-reco-content content__default"><p>In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine translation systems, and alleviated the need for Recurrent Neural Networks and LSTMs to derive context and meaning for sequence to sequence modeling. Since the <em> Attention Is All You Need </em> paper was published in 2017, there have been many experimental application and fine-tuning improvements made upon the original model. The latest such improvement is the Generative Pre-Trained Transformer 3, or GPT-3.</p> <p>GPT-3 is the third generation of the autoregressive language modeling GPT-n series created by OpenAI. Its architecture is exactly the same as the previous generation, GPT-2, with the exception of using alternating dense and locally banded sparce attention patterns. For this generation, OpenAI trained 8 different models with a variable number of parameters, layers, dimensions, learning rates, and batch sizes. Furthermore, unlike the GPT-2 which was trained on the WebText dataset, GPT-3 was trained on Common Crawl, which consists of nearly a trillion words. However, because this dataset is so large and unfiltered, various techniques had to be used to prevent overfitting the model and data contaimination.</p> <h2 id="novelty"><a href="#novelty" class="header-anchor">#</a> Novelty</h2> <p>Aside from the changes made above to the new model, the novelty of GPT-3 stems from its ability to use one-shot and few-shot demonstrations — it is not limited to zero-shot like GPT-2. To be more specific, this means that before actually testing GPT-3 on a specific task, the model can take a certain amount of examples <em> K </em> of the desired task as a form of conditioning before letting the model perform testing and inference on its own. For one-shot demonstrations, <em> K </em> is 1, and for few-shot demonstrations, <em> K </em> can be anywhere between 10 and 100 demonstrations of the task. This may sounds similar to fine-tuning a model, but there is one key difference: for these n-shot demonstrations, the model is <em> NOT </em> allowed to update its weights whereas for fine-tuning, the model is  <em> supposed </em> to update its weights. This novel approach of providing demonstrations before actual testing was specifically chosen because it best simulates human learning and behavior. Typically, when people are told to complete a task, they are usually offered one or more examples before having to attempt the task on their own. The table below illustrates the advantages and disadvantages of each of the different types of n-shots for GPT-3.</p> <table><thead><tr><th>Type</th> <th>Advantages</th> <th>Disadvantages</th></tr></thead> <tbody><tr><td>Fine Tuning</td> <td>Strong performance on a specific task</td> <td>The need for an new, large dataset for the specific task</td></tr> <tr><td>Few-Shot</td> <td>Major reduction in the need for task specific data and examples</td> <td>Potential for much worse performance than state-of-the-art fine tuned models</td></tr> <tr><td>One-Shot</td> <td>Most closely related to the way tasks are communicated to humans</td> <td>Performs worse than Few-Shot</td></tr> <tr><td>Zero-Shot</td> <td>Most convenient and no need for additional data</td> <td>Most challenging setting, humans cannot even perform some tasks without examples</td></tr></tbody></table> <h2 id="evaluation-on-various-tasks"><a href="#evaluation-on-various-tasks" class="header-anchor">#</a> Evaluation on Various Tasks</h2> <p>With these new types of testing and inference methods, GPT-3 was evaluated on a variety of NLP tasks that include text translation, generation, prediction, and comprehension. This fact alone — that one single model is able to be evaluated on a range of tasks — shows how powerful and exciting GPT-3 is. Prior to GTP-3, models had to be explicitly trained to do a specific task. In the following subsections, when describing the evaluations and accuracy for the different use cases, you will see that GPT-3 is powerful enough to match or even outperform some state-of-the-art models for NLP tasks.</p> <h3 id="translation"><a href="#translation" class="header-anchor">#</a> Translation</h3> <p>One of the tasks GPT-3 was tested on was its ability to perform sequence to sequence translation. The Common Crawl dataset OpenAI used to train their model contained English words 93% of the time and words from foreign languages 7% of the time. The words from the foreign natural languages were seamlessly blended together with the English text on a word, sentence, and document level. This allowed for the model to extract meaning and context from the foreign words, and be able to accurately translate them when asked to do so. From this translation test, GPT-3 resulted in a similar BLEU score compared to prior unsupervised NMT work. While zero-shot underperformed compared to prior works, one-shot improved the BLEU score by 4 and few-shot improved the BLEU score by 11. BLEU score improvement was especially noticed when translating phrases into English as opposed to translating sentence from English. In fact, some translations from English even outperform the best supervised results. The table and graph below shows the comparison of BLEU scores for each of the different n-shots and other NMT models, and how BLEU score changes with model size.</p> <p><img src="bleu_table.png" alt=""></p> <p><img src="bleu_graph.png" alt=""></p> <h3 id="word-prediction-and-text-generation"><a href="#word-prediction-and-text-generation" class="header-anchor">#</a> Word Prediction and Text Generation</h3> <p>Another task GPT-3 was tested on was predicting the next word given a few sentences or a passage. A demonstration along the lines of <em> Alice was friends with Bob. Alice went to visit her friend __. → Bob </em> was given to the model as an example along with a task description. From examples like these, the largest GPT-3 model (with 175 billion parameters) outperformed the current state-of-the-art model by 20%, predicting the next word correctly 86.4% of the time. Even if a model was trained on this specific task  and outperformed a state-of-the-art model by 20%, it would still be extraordinary.</p> <p>Another simliar test using GPT-3 was news article generation. The model was given a number of news articles written by humans as conditioning, and asked to generate a news article itself. Because this task is difficult to measure quantitatively, they measured the performance of the task based on whether or not a human could distinguish GPT-3 generated articles from human written one. As expected, as the size of the model increased, the ability for humans to distinguish the difference between a generated article and a human written one decreased. Astoundingly though, the mean human accuracy for detecting generated articles for the largest GPT-3 model was just slightly above chance at 52%.</p> <h3 id="comprehension"><a href="#comprehension" class="header-anchor">#</a> Comprehension</h3> <p>One of the most extensive tasks GPT-3 was trained on was its ability to comprehend and extract meaning from text. This was tested in a varitey of ways including utilizing new words, answering questions based on prompts and facts, and general reasoning.</p> <p>To evaluate the model's ability to utilize new words, the OpenAI team came up with some made up words and provided its definition to the model. From this information, the model was tasked to use the word in a sentence. Since there is no quantitative metric to measure how well a made-up word is used in a sentence, they again used a qualitative human evaluation. In most cases, the GPT-3 generated sentences that included the made-up word seemed to be proficient at using the novel word correctly. A similar test with a quantitative metric had to do with recognizing novel patterns by using SAT analogies. The model was provided SAT style analogy questions and tasked to determine which pair of words in the answer choices had the same relationship as the words in the question. In this task, GPT-3 was correct 65.2% of the time in the few-shot setting. While this may not seem too accurate, take into account that the average college applicant was correct only 57% of the time.</p> <p>Another comprehension task GPT-3 was tested on was knowledge-based question answering. The model was given the PhysicalQA dataset which contains a set of common sense questions about how the world works. In this experiment, GPT-3 achieved an accuracy of 82.8% in the few-shot setting on the easy questions. Compared to a previous fine-tuned state-of-the-art model RoBERTa, GPT-3's accuracy was 3.4% higher, again showcasing its ability to outperform fine-tuned models.</p> <p>The last comprehension test I wanted to mention had to do with relationships between sentences and pronoun references. GPT-3 was tested to see if it could determine which subject a pronoun was referring to. This Winograd Style task is a baseline task in natural language processing, where pronoun references are very clear to humans but ambiguous in grammatical context. Fine-tuned models designed for this task achieve almost perfect accuracy now, but GPT-3 still only falls a few points below state-of-the-art models at 88.6%. A similar task has to do with natural language inference between two sentences. Here, the model was tested to understand the relationship between two sentences. Usually, this involves determining whether or not a sentence can logically follow a given sentence. This task was evaluated on SuperGLUE's RTE dataset, where GPT-3's accuracy performed just slightly above random at 56%. I mention this test to show that while GPT-3 is a very powerful model, it cannot be generalized to every aspect of natural language processing.</p> <h2 id="case-study"><a href="#case-study" class="header-anchor">#</a> Case Study</h2> <p>As an experiment, I decided to test the BLEU score when translating a few phrases from Slovak, my native language, into English. Since Slovak isn't a particularly highly studied language in terms of machine translation, I thought it would be interesting to see if GPT-3 could accurately translate Slovak into English. In order to test this, I came up with a set of phrases in Slovak and their respective translations into English using Google Translate. From this set of translations, I took the Slovak phrases, fed them through the GPT-3 model completion task with a few-shot prompt, and did a BLEU score evaluation from the output of the GPT-3 model and the Google Translate reference text. One of the things I noticed throughout this experiment was that the model sometimes generates different translated sentences for the same input. Nevertheless, the BLEU score still hovers around the same values. The prompt and results are shown below. The average BLEU score between GPT-3's translation and Google Translate for the ten translations was 0.7196.</p> <p><a href="https://github.com/alexrasla/GPT-3-Blog" target="_blank" rel="noopener noreferrer">Case Study Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Prompt:
<em><br>
Slovak: Dobré ráno!
<br>
English: Good morning!
<br>
Slovak: Ako sa máš?
<br>
English: How are you?
<br>
Slovak: Odkiaľ si?
<br>
English: Where are you from?
<br>
Slovak:
<br></em></p> <table><thead><tr><th>Slovak (Input)</th> <th>Reference (Google Translate)</th> <th>Candidate (GPT-3)</th> <th>BLEU Score</th></tr></thead> <tbody><tr><td>Dnes si môžeme ísť kúpiť oblečenie do nákupného centra, ale nebudeme sa tam môcť najesť</td> <td>Today we can go to the mall to buy clothes, but we will not be able to eat there</td> <td>Today we can go to the mall to buy clothes, but we cannot eat there</td> <td>0.6313</td></tr> <tr><td>O druhej poobede budem doma na obed</td> <td>I will be home for lunch at two in the afternoon</td> <td>I'll be home for lunch at 2 o'clock</td> <td>0.3553</td></tr> <tr><td>Môžete mi poslať e-mail s dátumom a časom stretnutia?</td> <td>Can you send me an email with the date and time of the meeting?</td> <td>Can you send me an email with the date and time of our meeting?</td> <td>0.8555</td></tr> <tr><td>Keď sa skončí hokej, mali by sme ísť hrať hokej</td> <td>When hockey is over, we should go play hockey</td> <td>When hockey ends we should go play hockey</td> <td>0.5247</td></tr> <tr><td>Kúpim chladničku za sto dolárov</td> <td>I'll buy a refrigerator for a hundred dollars</td> <td>I want to buy a refrigerator for $100</td> <td>0.3457</td></tr> <tr><td>Musím sa učiť na test z biológie</td> <td>I have to study for a biology test</td> <td>I have to study for a biology test</td> <td>1.0</td></tr> <tr><td>Poďme do parku a zahrajme si futbal</td> <td>Let's go to the park and play football</td> <td>Let's go to the park and play football</td> <td>1.0</td></tr> <tr><td>Môžeme mať na večeru kuracie mäso?</td> <td>Can we have chicken for dinner?</td> <td>Can we have chicken for dinner?</td> <td>1.0</td></tr> <tr><td>Tento víkend som išiel do domu svojich priateľov a hral videohry</td> <td>This weekend I went to my friends' house and played video games</td> <td>This weekend I went to my friends' house and played video games</td> <td>1.0</td></tr> <tr><td>Veľmi sa teším, keď ich zajtra uvidím hrať na koncerte</td> <td>I'm very happy to see them play at the concert tomorrow</td> <td>I'm very looking forward to seeing them play at the concert tomorrow</td> <td>0.4833</td></tr></tbody></table> <h2 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> Conclusion</h2> <p>From this blog, we see that GPT-3 is extremely powerful and can perform better on some NLP tasks than even the state-of-the-art fine-tuned models. GPT-3 is not trained to do anything specific task; rather, it is pretrained on a massive dataset consisting of over a trillion words. When used for a specific task, the model is able to take zero, one, or a few demonstrations of the task depending on the type of experiment you want to conduct. This is ultimately what gives it the flexiblity to be applied to a variety of different domains within NLP.</p> <p>For language translation specifically, it achieved BLEU scores slightly below that of some supervised NMT models. This is incredible, given it is not fine-tuned or trained for machine tranlation. When increasing from zero-shot to one-shot to few-shot, GPT-3 performed significantly better and better in translating between languages. It performed best when translating to English because the CommonCrawl dataset used to train GPT-3 contained English words 93% of the time, and foreign words 7% of the time. In my specific case study of translating Slovak into English, the model achieved an average BLEU score of 0.7196 using a few-shot setting.</p> <p>In my opinion, even though GPT-3 is the latest and greatest language model to date, there is still room for improvement for these novel large-scale models. While GPT-3 proved that one pre-trained model can perform many NLP tasks and generate impressive results, it still lacks the ability to extract meaning and comprehension from text. Its synthesis techniques are gramatically sound and readable, but its contextual meaning is often repetitive, incoherent, and occasionaly contradictory. There are hypotheses that bidirectional encoder models could solve these sort of comprehension issues and create another incredible model, but until we figure out a way to truly extract meaning and generate long text that gives us coherent information, I believe GPT-3 will remain the status qou for generative language models.</p> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p>[1]<a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer"><em> Language Models are Few-Shot Learners </em><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>[2]<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer"><em> Attention Is All You Need </em><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">5/25/2022, 3:25:59 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#novelty" class="sidebar-link reco-side-novelty" data-v-cb1513f6>Novelty</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#evaluation-on-various-tasks" class="sidebar-link reco-side-evaluation-on-various-tasks" data-v-cb1513f6>Evaluation on Various Tasks</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#translation" class="sidebar-link reco-side-translation" data-v-cb1513f6>Translation</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#word-prediction-and-text-generation" class="sidebar-link reco-side-word-prediction-and-text-generation" data-v-cb1513f6>Word Prediction and Text Generation</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#comprehension" class="sidebar-link reco-side-comprehension" data-v-cb1513f6>Comprehension</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#case-study" class="sidebar-link reco-side-case-study" data-v-cb1513f6>Case Study</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#conclusion" class="sidebar-link reco-side-conclusion" data-v-cb1513f6>Conclusion</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/gpt/#references" class="sidebar-link reco-side-references" data-v-cb1513f6>References</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.c1ef5d59.js" defer></script><script src="/blog/assets/js/3.c983ed68.js" defer></script><script src="/blog/assets/js/1.7afedd4b.js" defer></script><script src="/blog/assets/js/29.0b766ca8.js" defer></script>
  </body>
</html>
