<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Can Visual Imagination Help Machine Translation? | Learning and Language Blog</title>
    <meta name="generator" content="VuePress 1.9.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.b14e6ae4.css" as="style"><link rel="preload" href="/blog/assets/js/app.e8b991d5.js" as="script"><link rel="preload" href="/blog/assets/js/3.3f842290.js" as="script"><link rel="preload" href="/blog/assets/js/1.7afedd4b.js" as="script"><link rel="preload" href="/blog/assets/js/24.5c85b0e6.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.c95eb3b0.js"><link rel="prefetch" href="/blog/assets/js/11.d13999ba.js"><link rel="prefetch" href="/blog/assets/js/12.dc26994e.js"><link rel="prefetch" href="/blog/assets/js/13.94479ba1.js"><link rel="prefetch" href="/blog/assets/js/14.0784e233.js"><link rel="prefetch" href="/blog/assets/js/15.cfd70764.js"><link rel="prefetch" href="/blog/assets/js/16.f69a7175.js"><link rel="prefetch" href="/blog/assets/js/17.40ceeeb7.js"><link rel="prefetch" href="/blog/assets/js/18.208d7283.js"><link rel="prefetch" href="/blog/assets/js/19.b27062cb.js"><link rel="prefetch" href="/blog/assets/js/20.7885ba38.js"><link rel="prefetch" href="/blog/assets/js/21.3591ca33.js"><link rel="prefetch" href="/blog/assets/js/22.47e4b9b9.js"><link rel="prefetch" href="/blog/assets/js/23.91fdf84c.js"><link rel="prefetch" href="/blog/assets/js/25.7a3a41b4.js"><link rel="prefetch" href="/blog/assets/js/26.18b16ac3.js"><link rel="prefetch" href="/blog/assets/js/27.c52489e4.js"><link rel="prefetch" href="/blog/assets/js/28.8cab24c3.js"><link rel="prefetch" href="/blog/assets/js/29.fe5c0a46.js"><link rel="prefetch" href="/blog/assets/js/30.d5530a44.js"><link rel="prefetch" href="/blog/assets/js/31.618bbcf3.js"><link rel="prefetch" href="/blog/assets/js/4.ba7e4154.js"><link rel="prefetch" href="/blog/assets/js/5.5e89c566.js"><link rel="prefetch" href="/blog/assets/js/6.4c91610c.js"><link rel="prefetch" href="/blog/assets/js/7.a08c6510.js"><link rel="prefetch" href="/blog/assets/js/8.e95acc43.js"><link rel="prefetch" href="/blog/assets/js/9.ae01f480.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.b14e6ae4.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-130b300a><div data-v-130b300a><div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-130b300a data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Learning and Language Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-130b300a><header class="navbar" data-v-130b300a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Learning and Language Blog" class="logo"> <span class="site-name">Learning and Language Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-130b300a></div> <aside class="sidebar" data-v-130b300a><div class="personal-info-wrapper" data-v-39576ba9 data-v-130b300a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    Lei Li
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>21</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>39</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Can Visual Imagination Help Machine Translation?</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-130b300a><main class="page"><section><div class="page-title"><h1 class="title">Can Visual Imagination Help Machine Translation?</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Yujie Lu</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/21/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Imagination</span><span class="tag-item" data-v-f875f3fc>Visual Machine Translation</span><span class="tag-item" data-v-f875f3fc>ImagiT</span></i></div></div> <div class="theme-reco-content content__default"><p>Machine translation has helped people daily life, and is also an important research topic especially in computer science community.
It expands from one language translate to another language, speech translate to text, etc.
Today, I'm going to talk about a paper &quot;Generative Imagination Elevates Machine Translation&quot;. I'll cover the background, challenge and motivation behind this paper. Then I'll go through some technical details of this paper as well as some in-depth analysis of their experimental settings and results. Finally, we will discuss about the potential extension of this work. Hopefully, this would give you a better understanding of this area, and point out to a promising research direction.</p> <p>Below is an example of multimodal machine translation, which is try to caption the image in two language and use them as paired translation.
<img src="/blog/assets/img/translation.4d1242ba.png" alt="Multimodal Machine Translation">
Everything sounds like a very straightforward application of ideas that are around for quite a while, but it has a secrete ingredient that makes it work efficiently. It is the way they represent the image that they call “Visual-Semantic Embedding”. They run an object detector for the images and represent each object by the penultimate layer of the object detection network. Then, they learn a projection of the object representations such that hidden states of the text encoder can be expressed as a linear combination of the projected object representations.
Compared to the standard unsupervised translation that relies on iterative back-translation, this way of training improves the translation quality quite a lot. On the other hand, the standard unsupervised methods are designed to work with much larger datasets and these data come from a quite narrow domain. When they integrate this training machinery into a supervised learning setup in addition to standard training examples, they reach the state-of-the-art results.</p> <h2 id="_1-background"><a href="#_1-background" class="header-anchor">#</a> 1. Background</h2> <p>Multimodal translation has benn explored for a while after the successful deep learning applcation for both natural language processing and computer vision.
As you can see in below images from the paper, is the problem setup of their proposed <strong>ImagiT</strong>. It is different from existing multimodal NMT, as they only takes sentence in the source language as the usual NMT task. They are trying to generating an image and utilize the internal visual information to help machine translation task.
<img src="/blog/assets/img/MultimodalNMT.f88c1546.png" alt="Multimodal NMT"></p> <h3 id="_1-1-challenge"><a href="#_1-1-challenge" class="header-anchor">#</a> 1.1 Challenge</h3> <p>Current state-of-the-art methods are built upon solely textual information, which is not consistent with how human try to implement the tasks. Most of the time, human would leverage some understanding or memory from visual information to help translation process.
How to leverage such visual information to improve NMT task might help it bridge the gap with human performance in multiple domains of machine translation.
But this is a really challenging task, given that the paired images of texts are scarce and synthesized images from textural input are of low quality.</p> <h3 id="_1-2-motivation"><a href="#_1-2-motivation" class="header-anchor">#</a> 1.2 Motivation</h3> <p>Existing multi modal NMT need the paired image as the input, which limit its development, since such bilingual parallel corpora annotated by images are not always available.
And the cost of manual annotation is pretty high.
This paper try to avoid the need of paired image by synthesizing it, thus address the bottlenecks mentioned above.</p> <h2 id="_2-methodology"><a href="#_2-methodology" class="header-anchor">#</a> 2. Methodology</h2> <p><strong>ImagiT</strong> is composed of roughly three key components, including imagination, visual representation module and common natural language translation model.
The imagination are usually generated by some GAN techniques, and the visual representation are either extracted from the hidden states of GAN or by extracting features with computer vision SOTA backbones over the generated images.
Then the visual and textual informatoin are fused together for the next step natural language translation task.</p> <h3 id="_2-1-problem-definition"><a href="#_2-1-problem-definition" class="header-anchor">#</a> 2.1 Problem Definition</h3> <p>The problem here is given a source language, we want to translate it into a target language in textual modal. In the meantime, instead of gathering paired images, we synthesize the imagination from textual input. Then the imagination is leveraged as the external knowledge, which contains some spatial correspondence to help further NMT task.</p> <h3 id="_2-2-overview"><a href="#_2-2-overview" class="header-anchor">#</a> 2.2 Overview</h3> <p>ImagiT embodies the encoder-decoder structure for end-to-end machine translation. Between the encoder and the decoder, there is an imagination step to generate semanticequivalent visual representation. Technically, the model is composed of following modules: source text encoder, generative imagination network, image captioning, multimodal aggregation and decoder for translation.
<img src="/blog/assets/img/architecture.8918a35e.png" alt="Overview"></p> <h3 id="components"><a href="#components" class="header-anchor">#</a> Components</h3> <p>For source text encoder, they utilize the state-of-art Transformer-based machine translation framework.
For generativ imagination network, they follow previous common practice of using the conditioning augmentation to enhance robustness to small perturbations along the conditioning text manifold and improve the diversity of generated samples with Generative Adversarial Network.
For an imagined image, its semantic meaning is equivalent to the source sentence. They leverage the image captioning to translate the imagined visual representation back to the source language. Such symmetric structure can make the imagined visual feature act like a mirror, which effectively enhancing the semantic consistency of the imagined visual feature and precisely reflect the underlying semantics.
Then comes to the aggregation part, to finally get the imagined visual textual representation, we have to fuse the representation from both modalities.
By inducing the hidden representation under the guide of image-aware attention and graph persepctive of Transformer would ben elegant method to integrate visual feature.
To train the whole network end-to-end, they leverage adversarial training to alternatively train the generator and the discriminator.</p> <h2 id="_3-experiments"><a href="#_3-experiments" class="header-anchor">#</a> 3. Experiments</h2> <h3 id="_3-1-datasets"><a href="#_3-1-datasets" class="header-anchor">#</a> 3.1 Datasets</h3> <p>They evaluate the proposed ImagiT model on two datasets, Multi30K and Ambiguous COCO. To show its ability to train with external out-of-domain datasets, they adopt MS COCO in the next analyzing section.
And the baseline is the conventional text-only Transformer.</p> <h3 id="_3-2-configuration"><a href="#_3-2-configuration" class="header-anchor">#</a> 3.2 Configuration</h3> <p>For the imagination network, the noise vector’s dimension is 100, and the generated visual feature is 128 × 128. The upsampling and residual block in visual feature transformers consist of 3×3 stride 1 convolution, batch normalization, and ReLU activation. For detailed experimental settings, please refer to the paper.</p> <h3 id="_3-1-results"><a href="#_3-1-results" class="header-anchor">#</a> 3.1 Results</h3> <p><img src="/blog/assets/img/experiments.d095be72.png" alt="Experiments">
As can be seen from above table, the main result from the Test2016, Test2017 for the En⇒De and En⇒Fr MNMT task is reported. The first category (Multimodal Neural Machine Translation Systems) collects the existing MNMT systems, which take both source sentences and paired images as input. The second category illustrates the systems that do not require images as input. Since our method falls into the second group, the baselines are the text-only Transformer and the aforementioned works.</p> <h2 id="_4-conclusion-and-discussin"><a href="#_4-conclusion-and-discussin" class="header-anchor">#</a> 4. Conclusion and Discussin</h2> <p>Generally, this paper proposed a novel method to generate semantic-consistent visual representations for imagination-guided translation without any anootated images.
But <strong>ImagiT</strong> is not applicable to larger-scale text-only NMT tasks, such as WMT'14, as they are not easy to be visualized, containing fewer objects and visually depictable entities. Besides, it largely depends on the quantity and quality of annotated images.
Future work might be combining with knowledge based imagination for more complex and abstract textual information.</p> <h2 id="reference"><a href="#reference" class="header-anchor">#</a> Reference</h2> <ul><li>Quanyu Long, Mingxuan Wang, Lei Li. Generative Imagination Elevates Machine Translation. NAACL 2021. <a href="https://arxiv.org/abs/2009.09654" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2009.09654<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">5/25/2022, 4:09:36 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_1-background" class="sidebar-link reco-side-_1-background" data-v-cb1513f6>1. Background</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_1-1-challenge" class="sidebar-link reco-side-_1-1-challenge" data-v-cb1513f6>1.1 Challenge</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_1-2-motivation" class="sidebar-link reco-side-_1-2-motivation" data-v-cb1513f6>1.2 Motivation</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_2-methodology" class="sidebar-link reco-side-_2-methodology" data-v-cb1513f6>2. Methodology</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_2-1-problem-definition" class="sidebar-link reco-side-_2-1-problem-definition" data-v-cb1513f6>2.1 Problem Definition</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_2-2-overview" class="sidebar-link reco-side-_2-2-overview" data-v-cb1513f6>2.2 Overview</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#components" class="sidebar-link reco-side-components" data-v-cb1513f6>Components</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_3-experiments" class="sidebar-link reco-side-_3-experiments" data-v-cb1513f6>3. Experiments</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_3-1-datasets" class="sidebar-link reco-side-_3-1-datasets" data-v-cb1513f6>3.1 Datasets</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_3-2-configuration" class="sidebar-link reco-side-_3-2-configuration" data-v-cb1513f6>3.2 Configuration</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_3-1-results" class="sidebar-link reco-side-_3-1-results" data-v-cb1513f6>3.1 Results</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#_4-conclusion-and-discussin" class="sidebar-link reco-side-_4-conclusion-and-discussin" data-v-cb1513f6>4. Conclusion and Discussin</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/imigit/#reference" class="sidebar-link reco-side-reference" data-v-cb1513f6>Reference</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.e8b991d5.js" defer></script><script src="/blog/assets/js/3.3f842290.js" defer></script><script src="/blog/assets/js/1.7afedd4b.js" defer></script><script src="/blog/assets/js/24.5c85b0e6.js" defer></script>
  </body>
</html>
