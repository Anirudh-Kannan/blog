<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Neural Machine Translation with Monolingual Translation Memory | Learning and Language Blog</title>
    <meta name="generator" content="VuePress 1.9.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.acdc4f47.css" as="style"><link rel="preload" href="/blog/assets/js/app.1e476687.js" as="script"><link rel="preload" href="/blog/assets/js/3.2ef92242.js" as="script"><link rel="preload" href="/blog/assets/js/1.4a2adf96.js" as="script"><link rel="preload" href="/blog/assets/js/17.4308b200.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.14505170.js"><link rel="prefetch" href="/blog/assets/js/11.d4adc340.js"><link rel="prefetch" href="/blog/assets/js/12.c3707bf6.js"><link rel="prefetch" href="/blog/assets/js/13.b12c1fa7.js"><link rel="prefetch" href="/blog/assets/js/14.6b8658a9.js"><link rel="prefetch" href="/blog/assets/js/15.6ffbe258.js"><link rel="prefetch" href="/blog/assets/js/16.f43ed359.js"><link rel="prefetch" href="/blog/assets/js/18.751d108a.js"><link rel="prefetch" href="/blog/assets/js/19.48518c5e.js"><link rel="prefetch" href="/blog/assets/js/20.1eae6522.js"><link rel="prefetch" href="/blog/assets/js/21.9bc61d6d.js"><link rel="prefetch" href="/blog/assets/js/22.9286c618.js"><link rel="prefetch" href="/blog/assets/js/23.f43e6fc8.js"><link rel="prefetch" href="/blog/assets/js/24.0c9e0ca4.js"><link rel="prefetch" href="/blog/assets/js/25.c0cff29d.js"><link rel="prefetch" href="/blog/assets/js/26.a182b8e5.js"><link rel="prefetch" href="/blog/assets/js/27.1a5dbb38.js"><link rel="prefetch" href="/blog/assets/js/28.f9c0811c.js"><link rel="prefetch" href="/blog/assets/js/29.55181991.js"><link rel="prefetch" href="/blog/assets/js/30.148ad523.js"><link rel="prefetch" href="/blog/assets/js/31.2485d125.js"><link rel="prefetch" href="/blog/assets/js/4.ae2a30de.js"><link rel="prefetch" href="/blog/assets/js/5.897bac6b.js"><link rel="prefetch" href="/blog/assets/js/6.8a461025.js"><link rel="prefetch" href="/blog/assets/js/7.4372ae5f.js"><link rel="prefetch" href="/blog/assets/js/8.d0eaa22e.js"><link rel="prefetch" href="/blog/assets/js/9.d4d8a32c.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.acdc4f47.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-130b300a><div data-v-130b300a><div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-130b300a data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Learning and Language Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-130b300a><header class="navbar" data-v-130b300a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Learning and Language Blog" class="logo"> <span class="site-name">Learning and Language Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-130b300a></div> <aside class="sidebar" data-v-130b300a><div class="personal-info-wrapper" data-v-39576ba9 data-v-130b300a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    Lei Li
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>21</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>39</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-130b300a><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Neural Machine Translation with Monolingual Translation Memory</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-130b300a><main class="page"><section><div class="page-title"><h1 class="title">Neural Machine Translation with Monolingual Translation Memory</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Rajan Saini</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>12/8/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Translation Memory</span></i></div></div> <div class="theme-reco-content content__default"><p>Hello fellow readers! In this post, I would like to share a recent advance in the field of Machine Translation. Specifically, I will be presenting the paper <em>Neural Machine Translation with Monolingual Translation Memory</em> by Cai et al, which received one of the six distinguished paper awards from ACL 2021.</p> <p>Paper: <a href="https://aclanthology.org/2021.acl-long.567/" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.acl-long.567/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
Code: <a href="https://github.com/jcyk/copyisallyouneed" target="_blank" rel="noopener noreferrer">https://github.com/jcyk/copyisallyouneed<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="so-what-is-machine-translation"><a href="#so-what-is-machine-translation" class="header-anchor">#</a> So... what is Machine Translation?</h2> <p>You can probably guess from the name: over the last few decades, researchers have tried to get computers (a.k.a. <strong>machines</strong>) to <strong>translate</strong> between human languages. Their hard work has resulted in a plethora of products, like Google Translate and Microsoft Translator. I'm sure all of you have played with these tools before (if not, <a href="https://translate.google.com/" target="_blank" rel="noopener noreferrer">there's no time like the present<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>) and have tried inputting various kinds of sentences. Their translation quality is quite impressive, and you can get near-native translations in many contexts (e.g. simple sentences between widely-spoken languages). However, you also may have noticed that some languages have better performance than others. This difference is due to the amount of usable translation data available (we'll go over what this means below). One of the key challenges being worked on today is to bridge this gap between <em>low-resource</em> and <em>high-resource</em> languages.</p> <figure><img src="/blog/assets/img/english-spanish.0670c487.png"> <img src="/blog/assets/img/english-xhosa.c44c3127.png"> <figcaption align="center" style="font-size:1vh;">The Spanish output seems to be accurate, but it might be possible to improve the Xhosa translation (disclaimer - I do not speak Xhosa). The word &quot;recurrent&quot; is being translated into &quot;oluqhubekayo&quot;, which means &quot;ongoing&quot;. In order to better capture the spirit of recurrent neural networks, however, a word closer to &quot;self-referencing&quot; might be more appropriate. This example is intended to illustrate the need for better low-resource language performance in jargon-heavy contexts. </figcaption></figure> <h3 id="data-is-key"><a href="#data-is-key" class="header-anchor">#</a> Data is Key</h3> <p>In order to understand what translation data is and why it is crucial for a good machine translation (MT) system, we first need to understand how these systems work. All of the current state-of-the-art use special kinds of programs called <em>neural networks</em>, which are infamous for being able to approximate <em>any</em> mathematical function by looking at examples.</p> <figure><img src="/blog/assets/img/neural-network.18b53a7f.png"> <figcaption align="center" style="font-size:1vh;">In this case, the network is learning to approximate f(x) = x + 1 just by looking at set of examples. The end goal to make correct guesses for unseen inputs, like passing 1.5 and getting 2.5. </figcaption></figure> <p>If we can convert an English sentence into a list of numbers, and if we can do the same for a Spanish sentence, then in theory, it should be possible for the network to learn how to convert the numbers from one form into the other.  And if we can train it on a large number of sentence pairs, it just might learn the grammatical rules well enough to provide good translations for unseen inputs.</p> <figure><img src="/blog/assets/img/neural-machine-translation.120ecfe8.png"> <figcaption align="center" style="font-size:1vh;">Instead of a single number, the network is learning to map between entire sequences of numbers. This is still possible, and there are special network architectures optimized for &quot;sequence to sequence&quot; tasks. </figcaption></figure> <p>Generally, hundreds-of-thousands or millions of parallel sentences are needed for good performance. However, it is hard to obtain pure, parallel datasets of this size for low-resource languages.</p> <h3 id="monolingual-data"><a href="#monolingual-data" class="header-anchor">#</a> Monolingual Data</h3> <p>Even though low-resource languages like Xhosa may lack large amounts of parallel data, they still have vast amounts of untranslated text. If incorporated creatively, this <em>monolingual</em> (i.e. untranslated) text can also be used to help the network learn. Many strategies exist:</p> <ul><li><em>Back-translation</em> uses an okay-performing, reversed translation model to turn each sentence from the monolingual data into a synthetic (fake) parallel pair. We can then train the main model on this new parallel data to hopefully expose it to a wider variety of sentences.</li> <li>One could also use this data to  <em>pre-train</em> the model before training on the parallel data. During the pre-training, all one has to do is delete random words from the monolingual corpus (e.g. a Xhosa book) and train the model to fill in the blanks. If the model does this task successfully and then trains on the parallel data, it may make better translations (at least, according to results published in <em>Multilingual Denoising Pre-training for Neural Machine Translation</em>).</li></ul> <p>However, the main advance I will be sharing with you presents an entirely new way of using monolingual data. Specifically, it combines monolingual data with a mechanism called <em>Translation Memory</em>.</p> <h3 id="translation-memory"><a href="#translation-memory" class="header-anchor">#</a> Translation Memory</h3> <p>Before getting into what <em>Translation Memory</em> actually is, let me first motivate it a little. Intuitively, a good-performing translation program should be able to perform well in a wide variety of contexts. Specifically, it should be able to translate sentences from speeches, news articles, folk tales, research papers, tweets, random blog posts on machine translation, etc. However, if you want a generic, jack-of-all-trades model to make passable translations in all of these areas, you inevitably have to make some sacrifices. This is where the <em>infrequent words</em> problem comes in: words that are infrequently-encountered by the model during training get discarded as noise, reducing performance in specialized domains. For example, it could forget the word &quot;chromosome&quot; and translate a biology textbook incorrectly. This happens to humans too. If you're trying to become an expert at 10 topics in a short timespan, you may easily forget technical words crucial to each of the 10 topics. If one of those topics is the <em>Medieval History of Mathematics</em>, you may easily forget who <em>al-Khawarizmi</em> was, causing you to mis-attribute his discoveries. (It's even harder for a neural network, as it could never exploit the mnemonic connection between <em>algorithm</em> and <em>al-Khawarizmi</em> 😄).</p> <p>However, the good news is that computers can look things up extremely quickly, far more quickly than humans. Imagine if you were tested on the <em>Medieval History of Mathematics</em>, but you had the textbook's glossary with you. All of a sudden, if you were asked about <em>al-Khawarizmi</em>, you could provide the correct answer in the time that it takes you to look him up. <em>Translation Memory</em> essentially imbues the neural network with this same capability.</p> <p>Essentially, each time the network is asked to generate a word, it can reference the translation memory (a bilingual dictionary mapping words between the source and target languages) to provide a more nuanced translation. The original researchers who proposed this concept came up with a two-component model architecture. One component consists of a neural network that generates its own &quot;guess&quot; for the translation word-by-word. The other component, called a memory component, retrieves the translations for each source word from a large dictionary. These two proposals are then combined, so that the one with higher confidence is used in the final translation.</p> <p>(Note that everything in this section comes from <em>Memory-augmented Neural Machine Translation</em> [2]. If you want to learn more, go read their paper! I promise you, it's very interesting).</p> <h3 id="end-of-intro"><a href="#end-of-intro" class="header-anchor">#</a> End of Intro</h3> <p>Thank you so much for bearing with me through the introduction. I did my best to put these topics into understandable words, but I may have accidentally glossed over something without explaining. If you have any questions or feedback, please don't hesitate to reach out to me at rajansaini@ucsb.edu.</p> <h2 id="monolingual-translation-memory"><a href="#monolingual-translation-memory" class="header-anchor">#</a> Monolingual Translation Memory</h2> <p>Now, the moment we have all been waiting for has finally arrived. We can finally talk about what Monolingual Translation Memory is, how it exploits monolingual data, and what its implications are.</p> <h3 id="intuition"><a href="#intuition" class="header-anchor">#</a> Intuition</h3> <p>The original Translation Memory introduced above is quite powerful, but it has a few limitations:</p> <ul><li>The first is that it requires a parallel, bilingual dictionary. This means that when trying to come up with a translation for an unknown word, the model will try to translate it <em>directly</em> rather than use the entire sentence's context.</li> <li>In addition, it is impossible for the retrieval mechanism itself to adapt as the model trains (a dictionary cannot change, even though other words might be more relevant).</li></ul> <p>The new Monolingual Translation Memory is designed to solve both of these issues by:</p> <ul><li>Using entire aligned sentences instead of a word-to-word dictionary. Such a sentence-sentence dictionary would be prohibitively long for humans to read through, but a clever retrieval mechanism would make it usable for a computer program. Furthermore, they use another neural network called a &quot;parallel encoder&quot; to determine whether two sentences are translations of each other. This allows them to associate monolingual sentences with existing translations, exploding the number of possibilities! (If you're confused by this, don't worry; this will be explained in more detail in a section below)</li> <li>Making the retrieval mechanism <em>learnable</em>. This means that as the model trains on a parallel corpus, the retrieval mechanism should also be able to adapt itself. Specifically, it should learn to provide the most relevant translations from a large set of sentences (including sentences outside the original sentence-sentence dictionary).</li></ul> <h3 id="parallel-encoders"><a href="#parallel-encoders" class="header-anchor">#</a> Parallel Encoders</h3> <p>The main secret behind this advance is its usage of <em>parallel encoders</em>. These are neural networks (i.e. mathematical functions) that map sentences to their meaning. More precisely, they map sentence embeddings (the original sentence converted to numbers, see &quot;Data is Key&quot;) to an <em>encoding vector</em>. The hope is for sentences with the <em>same meaning</em> to have the <em>same encoding</em>, even if they are expressed differently. For example, a good encoder would give &quot;I find recurrent neural networks fascinating&quot; and &quot;recurrent neural networks are fascinating to me&quot; similar encodings.</p> <figure><img src="/blog/assets/img/siamese-network-diagram.80d7db46.jpg"> <figcaption align="center" style="font-size:1vh;">This concept being illustrated is also known as a Siamese network. They have been used successfully outside of machine translation, for tasks like handwritten signature verifiction and facial recognition.  </figcaption></figure> <p>We can also extend this idea across languages! We can have an encoder for each language that converts their sentences into a shared &quot;meaning space&quot;. More precisely, two sentences from different languages that share the same meaning should get mapped to similar encoding vectors:</p> <p><img src="/blog/assets/img/training-translation-memory.a63daf06.png" alt=""></p> <p>Before we train our main neural network, we first train the target and source encoders on parallel text. The goal is for them to output identical encodings when they have the same meaning and different encodings when their meaning is different. This is called the &quot;alignment step&quot;. Then we can run the target encoder over a large untranslated corpus to encode each sentence. Then, every time the user wants a new sentence translated, we can find the target sentence with closest meaning by comparing the encodings:</p> <p><img src="/blog/assets/img/monolingual-translation-memory.5656641e.jpg" alt=""></p> <p>Through this method, every sentence ever written in the target language can now be retrieved (at least in theory). The researchers that proposed this process found that searching for the most-similar encoding is equivalent to performing a Maximum Inner Product Search (MIPS). Fast algorithms that solve MIPS have already been discovered, so they can also be used for a speedy retrieval.</p> <h3 id="main-translation-model"><a href="#main-translation-model" class="header-anchor">#</a> Main Translation Model</h3> <p>The retrieval model described above will give native translations, since it searches through text originally written by humans. However, what about completely new inputs for which direct translations have never existed? In that case, we would still want to use a larger neural network to create novel translations. If we could somehow allow that network to have access to some of the best-matched sentences found by the retrieval model, this should give us the best of both worlds. In highly domain-specific cases, the retrieval model could pull up relevant words or context that the network can use.  The network could then produce a translation that matches the source sentence while taking advantage of these technical words.</p> <figure><img src="/blog/assets/img/mtm-architecture.8528563b.png"> <figcaption align="center" style="font-size:1vh;">This figure was taken from the original paper. The retrieval model is expected to output a list of sentence encodings (z1, z2, ...) and their similarities to the original input sentence (f(x,z1), f(x,z2), ...). This information is then fed into the main network (the Translation Model), along with the source sentence's encoding. </figcaption></figure> <p>In order to pass the encodings and retrieved sentences to the translation model, we can use a memory encoder and an attention mechanism. The memory encoder is a simple learnable matrix that maps each sentence to a new vector space (this section is a little more advanced, but &quot;learnable&quot; means that the matrix's weights will get adjusted during training) . I would guess that this is done so that the source sentence and retrieved sentences get mapped to the same vector space. This way, they can be meaningfully compared with and added to each other. After the retrieved sentences get transformed into memory embeddings, an attention mechanism combines them with the source sentences (scaled back by their confidences). I will not explain the full details behind how the attention mechanism works (<a href="https://towardsdatascience.com/the-intuition-behind-transformers-attention-is-all-you-need-393b5cfb4ada" target="_blank" rel="noopener noreferrer">this article<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> has a great explanation), but the intuitive idea is that it highlights the relevant items from the memory encoder based on the source embeddings. After that, a decoder network converts the attention scores into the final translation, word-by-word.</p> <h2 id="done-at-last"><a href="#done-at-last" class="header-anchor">#</a> Done! At last</h2> <p>Whew! That was a lot! Anyway, this is my possibly-confusing attempt at sharing one of the latest advances in Machine Translation. If you need any clarification (especially with the translation model above), definitely feel free to reach out! Otherwise, thank you so much for your patience to have made it this far.</p> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p>[1] Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu. Neural Machine Translation with Monolingual Translation Memory. ACL 2021.</p> <p>[2] Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, Andrew Abel. Memory-augmented Neural Machine Translation. EMNLP 2017.</p> <p>[3] Victor Zhou. “Neural Networks from Scratch.” <em>Victor Zhou</em>, Victor Zhou, 9 Feb. 2020, https://victorzhou.com/series/neural-networks-from-scratch/.</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">6/22/2022, 7:11:55 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#so-what-is-machine-translation" class="sidebar-link reco-side-so-what-is-machine-translation" data-v-cb1513f6>So... what is Machine Translation?</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#data-is-key" class="sidebar-link reco-side-data-is-key" data-v-cb1513f6>Data is Key</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#monolingual-data" class="sidebar-link reco-side-monolingual-data" data-v-cb1513f6>Monolingual Data</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#translation-memory" class="sidebar-link reco-side-translation-memory" data-v-cb1513f6>Translation Memory</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#end-of-intro" class="sidebar-link reco-side-end-of-intro" data-v-cb1513f6>End of Intro</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#monolingual-translation-memory" class="sidebar-link reco-side-monolingual-translation-memory" data-v-cb1513f6>Monolingual Translation Memory</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#intuition" class="sidebar-link reco-side-intuition" data-v-cb1513f6>Intuition</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#parallel-encoders" class="sidebar-link reco-side-parallel-encoders" data-v-cb1513f6>Parallel Encoders</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#main-translation-model" class="sidebar-link reco-side-main-translation-model" data-v-cb1513f6>Main Translation Model</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#done-at-last" class="sidebar-link reco-side-done-at-last" data-v-cb1513f6>Done! At last</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/nmt-translation-memory/#references" class="sidebar-link reco-side-references" data-v-cb1513f6>References</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.1e476687.js" defer></script><script src="/blog/assets/js/3.2ef92242.js" defer></script><script src="/blog/assets/js/1.4a2adf96.js" defer></script><script src="/blog/assets/js/17.4308b200.js" defer></script>
  </body>
</html>
