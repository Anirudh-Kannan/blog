<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation | Learning and Language Blog</title>
    <meta name="generator" content="VuePress 1.9.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.2ce87651.css" as="style"><link rel="preload" href="/blog/assets/js/app.ae3b9445.js" as="script"><link rel="preload" href="/blog/assets/js/3.ea89d1a0.js" as="script"><link rel="preload" href="/blog/assets/js/1.207c5c8e.js" as="script"><link rel="preload" href="/blog/assets/js/24.15d596c7.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.cf2cf4f3.js"><link rel="prefetch" href="/blog/assets/js/11.18da651d.js"><link rel="prefetch" href="/blog/assets/js/12.34a58d02.js"><link rel="prefetch" href="/blog/assets/js/13.59721a5d.js"><link rel="prefetch" href="/blog/assets/js/14.8761e4c8.js"><link rel="prefetch" href="/blog/assets/js/15.4ba9bdae.js"><link rel="prefetch" href="/blog/assets/js/16.45147782.js"><link rel="prefetch" href="/blog/assets/js/17.8982a424.js"><link rel="prefetch" href="/blog/assets/js/18.c96397df.js"><link rel="prefetch" href="/blog/assets/js/19.939cb684.js"><link rel="prefetch" href="/blog/assets/js/20.e71f4699.js"><link rel="prefetch" href="/blog/assets/js/21.e4a2e4f2.js"><link rel="prefetch" href="/blog/assets/js/22.496a737e.js"><link rel="prefetch" href="/blog/assets/js/23.ac5f3fd2.js"><link rel="prefetch" href="/blog/assets/js/25.a00ea380.js"><link rel="prefetch" href="/blog/assets/js/26.6fb9d4fc.js"><link rel="prefetch" href="/blog/assets/js/27.b825cdbe.js"><link rel="prefetch" href="/blog/assets/js/28.5a2725b5.js"><link rel="prefetch" href="/blog/assets/js/29.c3596d81.js"><link rel="prefetch" href="/blog/assets/js/30.4b21a098.js"><link rel="prefetch" href="/blog/assets/js/4.a581a1a5.js"><link rel="prefetch" href="/blog/assets/js/5.ae985729.js"><link rel="prefetch" href="/blog/assets/js/6.6d597826.js"><link rel="prefetch" href="/blog/assets/js/7.ac4958aa.js"><link rel="prefetch" href="/blog/assets/js/8.48628bd2.js"><link rel="prefetch" href="/blog/assets/js/9.9dd8b84f.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.2ce87651.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1aefc0b4><div data-v-1aefc0b4><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1aefc0b4 data-v-1aefc0b4><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Learning and Language Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-1aefc0b4><header class="navbar" data-v-1aefc0b4><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Learning and Language Blog" class="logo"> <span class="site-name">Learning and Language Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1aefc0b4></div> <aside class="sidebar" data-v-1aefc0b4><div class="personal-info-wrapper" data-v-39576ba9 data-v-1aefc0b4><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    Lei Li
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>20</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>37</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLG/" class="nav-link"><i class="undefined"></i>
  NLG
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>Lei Li</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-1aefc0b4><main class="page"><section><div class="page-title"><h1 class="title">Break the Limitation of Training Data — A Better Encoder Enhanced by BERT for Speech Translation</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>Zichen Chen</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>11/30/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>Speech Translation</span><span class="tag-item" data-v-f875f3fc>BERT</span></i></div></div> <div class="theme-reco-content content__default"><p>Speech translation (ST) has increasing demand in our daily life and work. Applications like travel assistant, simultaneous conference translation and movie subtitling can highly reduce translation costs. Building a ST system that can understand and directly translate acoustic speech signals into text in a target language is challenging. For example, people do not always premeditate what they are going to say. Not like text translation, ST lacks completed organization sometimes. Another part is that the parallel corpus for ST is not enough, compared to the MT task. Especially, most ST methods are limited by the amount of parallel corpus.</p> <p>Such demanded applications stir interest within the research community. Over its decades history, speech translation has experienced several shifts in its primary research themes. The traditional way is coupled cascades of speech recognition and machine translation. Then, researchers emerged the exploration of tight coupling. Recently, end-to-end models have attracted much attention. To achieve end-to-end speech translation, our research community has made a lot of efforts. Today, we are going to introduce an interesting paper that helps us to break the limitations of the amount of parallel corpus.</p> <h2 id="end-to-end-st"><a href="#end-to-end-st" class="header-anchor">#</a> End-to-end ST</h2> <p>Generally, we use Cascaded ST systems to achieve ST, which extracts acoustic features and source-text semantic features before translating them to the target text.</p> <p><img src="/blog/assets/img/1.58403e0e.png" alt="Figure 1: The process of Cascaded ST systems"></p> <p>Figure 1: The process of Cascaded ST systems</p> <p>However, such intermediate stages meet challenges in real cases: no sufficient supervision to guide the encoder-decoder to process the audio properly, and the amount of parallel ST training corpus is small. It may generate errors when processing the audio inputs, and then influence the quality of MT.</p> <p>Therefore, end-to-end ST has been proposed to solve these issues. Methods like pre-training and multi-task learning can significantly improve performance. However, the performance of end-to-end ST is not as good as Cascaded ST for a very long time, since the ST data is insufficient to build a high-quality model.</p> <p>How to utilize the best of existing ST data to improve the end-to-end ST performance? <em>Listen, Understand and Translate (LUT): Triple Supervision Decouples End-to-end Speech-to-text Translation</em> introduces a human-inspired method to complete ST tasks via understanding acoustic information at the semantic level as much as possible, just like humans. It can guide the acoustic encoder to extract information from the auditory input. Additionally, it utilizes a pre-trained BERT model to enforce the upper encoder to produce as much semantic information as possible, without extra data.</p> <h2 id="inspired-by-human-lut"><a href="#inspired-by-human-lut" class="header-anchor">#</a> Inspired by Human — LUT</h2> <p>Let's take a look at the human's translation process. For example, when we want to translate Chinese audio to English, we need to listen to the audio first and try to understand the meaning of audio. After processing it in our brain, we can get the outputs.</p> <p><img src="/blog/assets/img/2.9d201401.png" alt="Figure 2: The process of how humans translate."></p> <p>Figure 2: The process of how humans translate.</p> <p>Currently, end-to-end models are built with an encoder-decoder architecture that can directly translate speech without using explicitly generated intermediate ASR output.  Methods like multi-task training and pre-training are used to incorporate additional ASR and MT data and reduce dependency on scarce end-to-end data. However, these techniques were not able to exploit ASR and MT data as effectively as Cascade methods, to get comparable translation quality.</p> <p>What if we make the machine to imitate human behaviour in translation? Can we improve the quality of speech translation? The answer is yes. The significant part of LUT is it adds the &quot;understand&quot; function in ST.</p> <p>LUT uses an <strong>acoustic encoder</strong> to &quot;Listen&quot;, a <strong>semantic encoder</strong> to &quot;Understand&quot;, and a <strong>translation decoder</strong> to &quot;Translate&quot;, to imitate the intermediate steps for effective end-to-end speech translation.</p> <p><img src="/blog/assets/img/3.72f0d233.png" alt="Figure 3: The architecture of LUT. "></p> <p>Figure 3: The architecture of LUT.</p> <p>Specifically, LUT consists of three modules:</p> <ul><li><strong>Acoustic encoder</strong> network that encodes the audio input sequence into hidden features corresponding to the source text;</li> <li><strong>Semantic encoder</strong> network that extracts hidden semantic representation for translation, which behaves like a normal machine translation encoder;</li> <li><strong>Translation decoder</strong> network that outputs sentence tokens in the target language.</li></ul> <p>The most interesting feature of LUT is utilizing Connectionist Temporal Classification (CTC) + distance Loss to optimize the encoders, and break the limitation of insufficient parallel ST data within BERT. This is particularly interesting because it is hard to train an end-to-end ST model well with a small number of data (compared to MT). This strategy helps LUT to learn semantic features, meanwhile, it can predict recognition results while predicting translation results. Thus, we can diagnose whether the wrong prediction for translation is caused by the wrong acoustic modelling.</p> <h2 id="evaluation-of-the-lut"><a href="#evaluation-of-the-lut" class="header-anchor">#</a> <strong>Evaluation of the LUT</strong></h2> <p>To test the effectiveness of LUT, the paper includes Augmented Librispeech English-French, IWSLT2018 English-German, and TED English-Chinese test.LUT achieves all the best performance. Compared to the Cascaded model, LUT is lighter (fewer parameters), which can perform a faster translation.</p> <h3 id="semantic-analysis"><a href="#semantic-analysis" class="header-anchor">#</a> Semantic Analysis</h3> <p>LUT compares the acoustic encoder and semantic encoder in this paper. Figure 3 shows the attention for different module layers. It is interesting that the acoustic encoder focuses on the local features, while the semantic encoder can capture more global features.</p> <p><img src="/blog/assets/img/4.03426352.png" alt="Figure 4: The visualization of attention for different module layers. (a), (b) visualize the attention of the last layer of the acoustic encoder and the first layer of semantic encoder respectively. "></p> <p>Figure 4: The visualization of attention for different module layers. (a), (b) visualize the attention of the last layer of the acoustic encoder and the first layer of semantic encoder respectively.</p> <p>It also compares the acoustic encoder and semantic encoder on the Fluent Speech Commands dataset, to test SpeakerVer (identify the speaker) and IntentIde (intention recognition). In Table 1, the semantic encoder only gets 46.3 accuracy on the SpeakerVer task, showing that the semantic encoder can focus on the translation task.</p> <table><thead><tr><th></th> <th>SpeakerVer</th> <th>IntentIde</th></tr></thead> <tbody><tr><td>Acoustic Encoder</td> <td>97.6</td> <td>91.0</td></tr> <tr><td>Semantic Encoder</td> <td>46.3</td> <td>93.1</td></tr></tbody></table> <p>Table 1: Classification accuracy on speaker verification and intent identification.</p> <p>It finds acoustic information is modelled at low-level layers and semantic information is captured at high-level layers.</p> <h3 id="lut-vs-cascaded-model"><a href="#lut-vs-cascaded-model" class="header-anchor">#</a> LUT vs Cascaded Model</h3> <table><thead><tr><th>Transcription</th> <th>LUT</th></tr></thead> <tbody><tr><td>reference</td> <td>it was mister jack maldon</td></tr> <tr><td>hypothesis</td> <td>it was mister jack mal</td></tr> <tr><td>Translation</td> <td></td></tr> <tr><td>reference</td> <td>c'était m. jack maldon</td></tr> <tr><td>hypothesis</td> <td>c'était m. jack maldon</td></tr></tbody></table> <table><thead><tr><th>Transcription</th> <th>Cascaded</th></tr></thead> <tbody><tr><td>reference</td> <td>it was mister jack maldon</td></tr> <tr><td>hypothesis</td> <td>it was mister jack mal</td></tr> <tr><td>Translation</td> <td></td></tr> <tr><td>reference</td> <td>c'était m. jack maldon</td></tr> <tr><td>hypothesis</td> <td>c'était m. jack mal</td></tr></tbody></table> <p>Here is an example of transcription and translation on En-Fr test set generated by the LUT and Cascaded model. It has a recognition error in the transcription stage (mal). LUT can translate correctly, ignoring the mistake that happened in transcription, while Cascaded Model can not correct the error and continue to pass the incorrect signal.</p> <p>LUT not only can directly translate to the target language within the original audio information, but also is fault-tolerant during the acoustic modelling.</p> <h1 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h1> <p>LUT is a unified training framework to decouple the end-to-end speech translation
task, under the supervision of the acoustic, semantic, linguistic level. It is effective and utilizes existing data to solve the problem of insufficient ST data.</p> <p>The code can be found at:  <a href="https://github.com/dqqcasia/st" target="_blank" rel="noopener noreferrer">https://github.com/dqqcasia/st<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://github.com/dqqcasia/st" target="_blank" rel="noopener noreferrer">GitHub - dqqcasia/st: End-to-end Speech Translation<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Paper: <a href="https://arxiv.org/pdf/2009.09704.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2009.09704.pdf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h1 id="reference"><a href="#reference" class="header-anchor">#</a> Reference</h1> <ol><li><p>Dong Q, Ye R, Wang M, et al. &quot; Listen, Understand and Translate&quot;: Triple Supervision Decouples End-to-end Speech-to-text Translation[J]. AAAI 2021.</p></li> <li><p>Sperber M, Paulik M. Speech Translation and the End-to-End Promise: Taking Stock of Where We Are[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 7409-7421.</p></li> <li><p>Stoian M C, Bansal S, Goldwater S. Analyzing ASR pretraining for low-resource speech-to-text translation[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 7909-7913</p></li> <li><p>Weiss R J, Chorowski J, Jaitly N, et al. Sequence-to-sequence models can directly translate foreign speech[J]. arXiv preprint arXiv:1703.08581, 2017.</p></li></ol> <h1 id="supplemental-materials"><a href="#supplemental-materials" class="header-anchor">#</a> Supplemental Materials</h1> <ol><li><p>Baevski A, Zhou H, Mohamed A, et al. wav2vec 2.0: A framework for self-supervised learning of speech representations[J]. arXiv preprint arXiv:2006.11477, 2020.</p></li> <li><p>Han C, Wang M, Ji H, et al. Learning Shared Semantic Space for Speech-to-Text Translation[J]. arXiv preprint arXiv:2105.03095, 2021.</p></li> <li><p>Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008.</p></li></ol></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">5/25/2022, 2:23:11 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lut/#end-to-end-st" class="sidebar-link reco-side-end-to-end-st" data-v-cb1513f6>End-to-end ST</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lut/#inspired-by-human-lut" class="sidebar-link reco-side-inspired-by-human-lut" data-v-cb1513f6>Inspired by Human — LUT</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lut/#evaluation-of-the-lut" class="sidebar-link reco-side-evaluation-of-the-lut" data-v-cb1513f6>Evaluation of the LUT</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lut/#semantic-analysis" class="sidebar-link reco-side-semantic-analysis" data-v-cb1513f6>Semantic Analysis</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/blogs/dl4mt/2021/lut/#lut-vs-cascaded-model" class="sidebar-link reco-side-lut-vs-cascaded-model" data-v-cb1513f6>LUT vs Cascaded Model</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.ae3b9445.js" defer></script><script src="/blog/assets/js/3.ea89d1a0.js" defer></script><script src="/blog/assets/js/1.207c5c8e.js" defer></script><script src="/blog/assets/js/24.15d596c7.js" defer></script>
  </body>
</html>
