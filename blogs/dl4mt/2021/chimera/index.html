<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Learning Shared Semantic Space for Speech Translation | MLNLP Blog</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on NLP, Machine Learning, Data Mining, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.0a66d5ca.css" as="style"><link rel="preload" href="/blog/assets/js/app.f1a218fc.js" as="script"><link rel="preload" href="/blog/assets/js/3.81641b16.js" as="script"><link rel="preload" href="/blog/assets/js/1.6e4b099c.js" as="script"><link rel="preload" href="/blog/assets/js/10.1c81b2ea.js" as="script"><link rel="prefetch" href="/blog/assets/js/11.b692c971.js"><link rel="prefetch" href="/blog/assets/js/12.b2e6cb72.js"><link rel="prefetch" href="/blog/assets/js/13.9521aae5.js"><link rel="prefetch" href="/blog/assets/js/14.4e983101.js"><link rel="prefetch" href="/blog/assets/js/15.e7958275.js"><link rel="prefetch" href="/blog/assets/js/16.52caed14.js"><link rel="prefetch" href="/blog/assets/js/17.b4efab4f.js"><link rel="prefetch" href="/blog/assets/js/18.2fa7c012.js"><link rel="prefetch" href="/blog/assets/js/19.a7cc6e8a.js"><link rel="prefetch" href="/blog/assets/js/20.a7c24616.js"><link rel="prefetch" href="/blog/assets/js/21.a7cd8218.js"><link rel="prefetch" href="/blog/assets/js/22.fc855139.js"><link rel="prefetch" href="/blog/assets/js/4.984de674.js"><link rel="prefetch" href="/blog/assets/js/5.05d493bf.js"><link rel="prefetch" href="/blog/assets/js/6.59d87037.js"><link rel="prefetch" href="/blog/assets/js/7.90237191.js"><link rel="prefetch" href="/blog/assets/js/8.29d46ec6.js"><link rel="prefetch" href="/blog/assets/js/9.c44200ab.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.0a66d5ca.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>MLNLP Blog</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>Blogs on NLP, Machine Learning, Data Mining, and other AI related topics</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Lei Li</span>
            
          <span data-v-4e82dffc>2016 - </span>
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="MLNLP Blog" class="logo"> <span class="site-name">MLNLP Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    Lei Li
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>12</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>17</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/ST/" class="nav-link"><i class="undefined"></i>
  ST
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/DL4MT/" class="nav-link"><i class="undefined"></i>
  DL4MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/IE/" class="nav-link"><i class="undefined"></i>
  IE
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Learning Shared Semantic Space for Speech Translation</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Lei Li</span>
            
          <span data-v-4e82dffc>2016 - </span>
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Learning Shared Semantic Space for Speech Translation</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>Xianjun Yang</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>11/13/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>Speech Translation</span></i></div></div> <div class="theme-reco-content content__default"><p>How to develop a translation model that can take both speech and text as input and translate to target language?
Can we borrow inspiration from human brain study to improve the speech translation models?</p> <p>Reading Time: About 15 minutes.</p> <p>Paper：<a href="https://arxiv.org/pdf/2105.03095.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2105.03095.pdf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Github: <a href="https://github.com/Glaciohound/Chimera-ST" target="_blank" rel="noopener noreferrer">https://github.com/Glaciohound/Chimera-ST<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p><img src="/blog/assets/img/c1.9586cb94.png" alt="image1"></p> <p>Although it seems difficult for normal people to acquire more than two languages, according
to the Wikipedia there are many polyglots who can speak tens of languages. For example,
Ziad Fazah, speaking a total of 59 world languages, is believed to be the world’s greatest living polyglot.
However, compared with others in the history his record does not stand out. It was recorded that
Cardinal Giuseppe Caspar Mezzofanti, who was born in 1774, could speak 38 languages and 40 dialects.
Meanwhile, the 10th-century Muslim polymath Al-Farabi was claimed to know 70 languages. The German Hans
Conon von der Gabelentz, born in 1807, researched and published books about grammars for 80 languages.
The highest record probably belongs to Sir John Bowring, Governor of Hong Kong from 1854 to 1859,
who was said to know 200 languages, and capable of speaking 100. But it turns out extremely difficult
for machine to learn natural languages like humans, in particular, despite its huge potential applications,
speech translation has always been a challenge for machine.
One common advantage is that those polyglots all benefit from not only text, but also audio corpus. However,
how to utilize both audio and text information to help machine speech translation has not been fully exploited.
The challenge comes from the intrinsic difference of modality between audio and text.</p> <p>A recent work Chimera from ByteDance AI Lab and UIUC aims to draw strengths from both modalities
for speech translation [1]. Their key idea is to represent text and audio inputs differently,
then fuse them together by projecting audio and text features into a common semantic representation
to boost the ST performance.</p> <p>There are two main advantages for including audio and text data together for training one ST model.
First, humans learn languages simultaneously from audio, text and videos rather than pure text. Inspired by
this observation, it is believed that text knowledge can provide additional insights for ST. Second,
since MT corpus is much larger compared with small corpus of ST, which is also expensive to create,
incorporating MT text provides much fruitful training data and is expected to yield improvements on ST
when bridging the modality gap properly.</p> <p>Taking those benefits into consideration, the Chimera model showed significant improvements by unifying MT and ST
tasks on the benchmark ST datasets containing more than ten languages pairs.</p> <p>Unlike previous translation models, Chimera has established a successful paradigm of
bridging the modality gap between text and audio for speech translation.
This is similar to multi-modality MT, in which images can improve the text translation quality. Considering
the pixel level information in images is accurately described, the audio is even noisier and leads to more challenges.</p> <p>Chimera is designed for an end-to-end speech-to-text translation task. It has two advantages.
First, the translation quality can be consistently improved by leveraging a large amount of external
machine translation data. In rich-resource directions, such as the largest speech translation dataset
MuST-C, which already contains translations from English to eight languages with each pair
consisting of at least 385 hours of audio recordings, Chimera can still significantly improve the
quality, reaching a new State-Of-The-Art(SOTA) BLEU score on all language pairs. In low-resource
directions, such as LibriSpeech dataset containing only 100 hours of training data, Chimera also
performs surprisingly well and consistently outperforms the previous best results.
Finally, they verified the common knowledge conveyed between these audio and text tasks indeed comes
from the shared semantic space and thus paves a new way for augmenting training resources across modalities.</p> <p>Chimera can be thought of as a multimodal in the field of speech translation in general.
When you need to develop a speech translation model but don't have enough audio data, you
may consider using Chimera, and it can turn out to be better than your expectation!
The research data, codes and resources are also kindly published by the authors.</p> <p>Next, we will introduce and analyze Chimera from three aspects: 1) the challenges of bridging the gap
between audio and text for speech translation; 2) the motivation and methods of Chimera; 3) the
performance and analysis of Chimera.</p> <h2 id="challenges-in-speech-translation"><a href="#challenges-in-speech-translation" class="header-anchor">#</a> Challenges in speech translation</h2> <p>The current paradigm of artificial intelligence highly depends on training on a large dataset, and thus
make predictions on a small test set. However, different modalities of data have been labeled for
various tasks but seldom been utilized together due to the modality gap of data representations. For example,
billions of parallel MT corpora have been ignored as additional training data for SP for a long time.
At the meanwhile, the data of ST is always in the dearth due to the difficulty of collection
and high cost. Looking at the relatively smaller amount of parallel data for ST compared with MT,
it is natural to have the idea of combining them together. Unfortunately, although they both encode human
languages, they are dissimilar in both coding attributes(pitch, volume, and intonation versus words,
suffixes, and punctuation) and length(thousands of time frames versus tens of words). Therefore, it has
always been a challenge to unify representations from audio and text. The recent evidence from functional
neuroimaging identifies certain regions in brain that the processing stream for speech sounds and visual
text correlates positively with the subjects' reading ability. This finding provides the intuition
for developing a multi-modality converged representation of audio and text in language activities. But
only little previous research explored this direction possibly due to the difficulties of modality fusion and
marginal improvements. Surprisingly, Chimera establishes a new bridge to fill the modality gap between
speech and text and can serve as a new foundation in this area.</p> <p><img src="/blog/assets/img/c3.4d337986.png" alt="image2"></p> <p>In the above figure[2], the color represents relative contribution of unimodal visual (green), unimodal auditory
(red), and similar contribution of visual and auditory predictors (yellow) to explaining signal variance.
The lateral and inferior occipital-temporal cortex were active by unimodal presentation of letters, while
Heschl's gyrus (primary auditory cortex) and portions of the superotemporal cortex were activated by
unimodal presentation of speech sounds. This neuroimaging evidence convinces the connection of audio and text in human
brain, serving as theoretical foundation of modality fusion.</p> <h2 id="motivation-and-techniques-of-chimera"><a href="#motivation-and-techniques-of-chimera" class="header-anchor">#</a> Motivation and Techniques of Chimera</h2> <p><img src="/blog/assets/img/c5.641e28dd.png" alt="image4"></p> <p>​For language learners, a very interesting phenomenon is that they learn better by audio and text
together rather than text only. The aforementioned famous polyglots also stated that their success
originated from various interactions with native speakers in other languages, such as listening,
reading, and speaking. Nowadays, many people also learn new languages by watching movies, in which
they will be immersed in audio and subtitles. In particular, the above figure is Ioannis Ikonomou, who was born
in Greek and now works as translator for European Union. Ikonomou can speak 32 languages and could also translate
those languages to each other in daily work. According to his interview, he was born in a famous tourism city,
where people from all over tha world visit there every day. Under the influence of different tourists,
he learned English at 5 years old, German at 7 years old, Italian at 10 years old, Russian at 13 years
old, East African Swahili at the age of 14, and Turkish at the age of 16. He said &quot;Learning Polish
can make Polish dumplings better. Learning Russian is to understand Dostoyevsky,
Persian is to appreciate ancient Persian poetry, and Hungarian is to understand Hungarian folk
songs. For German, it is to understand the veteran show &quot;Mirror of the World&quot; every Sunday evening.&quot;[3].
It is worthy to point that he learned most languages when he was still a child, demonstrating strong language ability
of human children.</p> <p><img src="/blog/assets/img/c6.8132692a.png" alt="image5"></p> <p>The above figure shows the developmental milestones of human children learners. Children first learns
how to speak from their family and thus how to speak fluently, read and write formally at school.</p> <p>All those evidence demonstrates that combining audio and text could help humans learn a new language.
A natural next step is to apply this idea in speech machine translation.</p> <p>The design goal of Chimera is based on such considerations: design a general framework
to learn the combination of audio and text from languages, and then it will benefit from this
combined pre-training when migrating to the new speech translation direction. Just like language
learners, after learning two modalities, the one modality becomes easier. The design of Chimera
follows three basic principles: first, learn the shared semantic memory module to bridge the gap
between the representation of audio and text; Second, the training objective of pre-training comes from three
parts: 1) the speech-to-text translation training, 2) the text machine translation training,
and 3) Bi-modal contrastive training; Third, train on external MT corpora and then apply the model
in ST task.</p> <h2 id="shared-semantic-memory"><a href="#shared-semantic-memory" class="header-anchor">#</a> Shared semantic memory</h2> <p><img src="/blog/assets/img/c7.d98bdca7.png" alt="image6">
Chimera follows a transformer based encoder-decoder framework and above is an overview. Word
embedding for text input and the Wav2vec2[4] sub-module for speech input
are both included in the Encoder Module. The shared semantic projection Module generates
semantic memory with fixed-size representation from contextual features using its memory query.
The Decoder Module decodes semantic memory translation.</p> <p>Since the encoder and decoder are actually standard modules based on transformer[5], which have been proven
to be the SOTA design in many natural language processing tasks, the shared semantic memory module is
the key to success of Chimera. So here we will mainly discuss the fantastic design of this shared module.
Among this framework, the module relies heavily on shared semantic projection. In fact, contextual elements of speech
and text can have a wide range of distributions and lengths. The shared semantic projection should,
in theory, compute a fixed number of semantic features as output semantic memories.
This module takes the contextual information extracted from the encoding module as input and
outputs semantic memories with a set length of m. It is made up of n layers of attention. It
stores a tuple of m trainable input-dependent memory queries as the initial &quot;memories&quot; to
represent the categories of required semantic information. Attention &quot;keys&quot; and &quot;values&quot; are
provided by unimodal contextual features, but attention &quot;queries&quot; are provided by memories.
Memories are fed into the n shared semantic projection layers in an iterative fashion,
with each layer's output being used as input to the next layer. The semantic memory is created
from the final output.</p> <h2 id="dataset-and-preprocessing"><a href="#dataset-and-preprocessing" class="header-anchor">#</a> Dataset and preprocessing</h2> <p>Two datasets were used for conducting experiments to verify the effectiveness of Chimera.
One is called MuST-C, the largest ST corpus, which contains translations from English(EN) to 8 languages: Dutch (NL), French (FR),
German (DE), Italian (IT), Portuguese (PT), Romanian(RO), Russian (RU), and Spanish (ES). With
each pair consisting of at least 385 hours of audio recordings. Another popular one is Augmented LibriSpeech Dataset
(En-Fr), which is composed of aligned e-books in French and their human reading in English of 100 hours.
They also incorporate data from WMT, OpenSubtitles and OPUS100 translation tasks as pretraining corpora.</p> <p>In practice, speech input, the 16-bit raw wave sequences are normalized by a factor of 215 to the range of
[-1, 1), which uses the Wav2Vec2 Module following the base configuration in [4].
The shared Transformer encoder consists of 6 layers. The memory queries are 64 512-
dimensional vectors. The parameters of shared semantic  projection resemble a 3-layer Transformer
encoder. The Transformer decoder has 6 layers. Each of these Transformer layers, except for those
in the Wav2Vec2 module, has an embedding dimension of 512, a hidden dimension of 512, and 8
attention heads.</p> <p>Chimera contains around 165M parameters. The whole training process for one trial on 8 Nvidia Tesla-V100 GPUs
generally takes 20 –40 hours according to the translation direction.</p> <h2 id="effectiveness-of-chimera"><a href="#effectiveness-of-chimera" class="header-anchor">#</a> Effectiveness of Chimera</h2> <p>In summary, Chimera has the following advantages:</p> <ol><li>New state-of-the-art performance on all language pairs</li></ol> <p>Even though they did not use Google Translate results on Augmented Librispeech as most baselines,
Chimera obtains state-of-the-art performance on all language pairs. Chimera's EN-DE results
use WMT14+OpenSubtitles for MT pretraining, whereas the original paper also contains a full ablation
research on the effect of MT data. It's worth noting that the improvement in EN-PT isn't as
dramatic as it is in EN-DE and EN-FR. This is due to a mismatch in data between OPUS100 and
MuST-C. OPUS100 has a high amount of sentences from movie subtitles, which are more informal,
feature repeated lines, and address issues that are not covered in MuST-C public speeches.</p> <ol start="2"><li>Successfully share knowledge across tasks</li></ol> <p>Additional trial findings corroborate their design of auxiliary tasks by demonstrating its
ability to acquire a well-structured shared semantic space as well as successfully exchange
learned knowledge between MT and ST.</p> <p>Here are some representative experimental results:</p> <h3 id="_1-benchmark-experiments"><a href="#_1-benchmark-experiments" class="header-anchor">#</a> 1. Benchmark Experiments</h3> <p>The below two tables demonstrate the main results on tst-COMMON subset on all 8 languages in MuST-C dataset
amd on LibriSpeech English-French dataset.</p> <p><img src="/blog/assets/img/c8.1445c55b.png" alt="image7"></p> <p>Table 1: Main results on tst-COMMON subset on all 8 languages in MuST-C dataset.</p> <p><img src="/blog/assets/img/c9.6ebbd6a2.png" alt="image8"></p> <p>Table 2: Results on LibriSpeech English-French dataset.</p> <h3 id="_2-visualizations"><a href="#_2-visualizations" class="header-anchor">#</a> 2. Visualizations</h3> <p><img src="/blog/assets/img/c10.22e60859.png" alt="image9"></p> <p>Regardless of the input modality, the shared semantic projection is designed to extract only
the semantic categories of information required for decoding. To validate this hypothesis, a
2-dimensional PCA projection was performed in the semantic memories across different samples.</p> <p>In the above figure, each colored cluster (circled out) represents a semantic memory element. A '.' corresponds to
a speech semantic memory, and a “+” marks a text one. It is obvious that semantic memories are
strongly clustered, with each individual learning a specific region. The model's capacity to
overlook representation disparities and bridge the modality gap is demonstrated by the close
distance of speech and text representations inside the same region.</p> <p><img src="/blog/assets/img/c11.d06ba500.png" alt="image9"></p> <p>One randomly selected semantic memory subspace was analyzed by PCA to its related cluster to get
a better look at the structure of each semantic memory subspace.</p> <p>The above figure is the visualization of one specific semantic memory with no different samples or
modalities. &quot;+&quot; denotes text representations, while &quot;.&quot; denotes speech representations. Marks
of the same color are linked by dashed lines and come from the same speech-transcript pair.
Some speech-transcript pairs have been circled and their transcripts have been annotated.
Three different fonts denote three different sets of transcripts with similar patterns.
As can be seen from the circles, the matched speech and transcript inputs are indeed close to each other.
Such results provide strong evidence of the efficacy of the semantic memory module, especially when
considering audio and text represent different modalities.</p> <h2 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h2> <p>Going back to the polyglots, who successfully learn new languages through various kinds of interactions
with the environments such as speaking, reading and writing, Chimera makes one important step towards
drawing strength from text machine translation to advance speech translation.
In general, Chimera unites MT and ST tasks by projecting audio and text data to a shared
semantic representation, boosting performance on ST benchmarks MuST-C and Augmented
Librispeech to a new state-of-the-art. Further visualizations show that the shared semantic space does indeed
convey common knowledge between these two tasks, paving the path for novel ways to supplement training materials across modalities.</p> <p>In the future, we are looking forward to more advanced techniques to solve two additional problems: 1) how to
tightly align speech and text representations and 2) how to make the workflows of MT and ST fully shared.
Since ST is an exciting new area, there are a lot of interesting research and progress almost every week.
In the near future, we believe a beautiful new world, where the real time speech translation comes true and the
language barriers among countries and nations are broken, is waiting fo us.</p> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p>[1] Han, Chi, Mingxuan Wang, Heng Ji, and Lei Li. &quot;Learning Shared Semantic Space for Speech-to-Text Translation.&quot; ACL 2021.</p> <p>[2] Van Atteveldt, Nienke, Elia Formisano, Rainer Goebel, and Leo Blomert. &quot;Integration of letters and speech sounds in the human brain.&quot; Neuron 43, no. 2 (2004): 271-282.</p> <p>[3] <a href="https://en.wikipedia.org/wiki/Ioannis_Ikonomou" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Ioannis_Ikonomou<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>[4] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems, volume 33, pages 12449–12460. Curran Associates, Inc.</p> <p>[5] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. &quot;Attention is all you need.&quot; In Advances in neural information processing systems, pp. 5998-6008. 2017.</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">12/6/2021, 10:46:59 AM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#introduction" class="sidebar-link reco-side-introduction" data-v-70334359>Introduction</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#challenges-in-speech-translation" class="sidebar-link reco-side-challenges-in-speech-translation" data-v-70334359>Challenges in speech translation</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#motivation-and-techniques-of-chimera" class="sidebar-link reco-side-motivation-and-techniques-of-chimera" data-v-70334359>Motivation and Techniques of Chimera</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#shared-semantic-memory" class="sidebar-link reco-side-shared-semantic-memory" data-v-70334359>Shared semantic memory</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#dataset-and-preprocessing" class="sidebar-link reco-side-dataset-and-preprocessing" data-v-70334359>Dataset and preprocessing</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#effectiveness-of-chimera" class="sidebar-link reco-side-effectiveness-of-chimera" data-v-70334359>Effectiveness of Chimera</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#_1-benchmark-experiments" class="sidebar-link reco-side-_1-benchmark-experiments" data-v-70334359>1. Benchmark Experiments</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#_2-visualizations" class="sidebar-link reco-side-_2-visualizations" data-v-70334359>2. Visualizations</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#summary" class="sidebar-link reco-side-summary" data-v-70334359>Summary</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/dl4mt/2021/chimera/#references" class="sidebar-link reco-side-references" data-v-70334359>References</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.f1a218fc.js" defer></script><script src="/blog/assets/js/3.81641b16.js" defer></script><script src="/blog/assets/js/1.6e4b099c.js" defer></script><script src="/blog/assets/js/10.1c81b2ea.js" defer></script>
  </body>
</html>
